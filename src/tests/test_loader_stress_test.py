from omnipkg.common_utils import safe_print

#!/usr/bin/env python3
"""
ğŸŒ€ OMNIPKG CHAOS THEORY - DAEMON EDITION ğŸŒ€
Now using the REAL worker daemon for maximum parallelism!
"""
import sys
import os
import subprocess
import time
import random
import threading
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path

# Import the daemon client
try:
    from omnipkg.common_utils import safe_print
    from omnipkg.loader import omnipkgLoader
    from omnipkg.isolation.worker_daemon import DaemonClient, WorkerPoolDaemon
except ImportError:
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))
"""
ğŸŒ€ OMNIPKG CHAOS THEORY ğŸŒ€
The most UNHINGED dependency isolation stress test ever conceived.
If this runs without exploding, we've broken the laws of Python itself.
âš ï¸  WARNING: This script is scientifically impossible. Run at your own risk.
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“¦ IMPORTS: The New Architecture
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
try:
    # 1. Common Utils
    from omnipkg.common_utils import safe_print, ProcessCorruptedException

    # 2. The Core Loader
    from omnipkg.loader import omnipkgLoader

    # 3. The New Isolation Engine (âœ¨ REFACTORED âœ¨)
    from omnipkg.isolation.runners import run_python_code_in_isolation
    from omnipkg.isolation.workers import PersistentWorker
    from omnipkg.isolation.switchers import TrueSwitcher
except ImportError:
    # Fallback for running directly without package installed
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))
    from omnipkg.common_utils import safe_print, ProcessCorruptedException
    from omnipkg.loader import omnipkgLoader
    from omnipkg.isolation.runners import run_python_code_in_isolation
    from omnipkg.isolation.workers import PersistentWorker

#  env vars globally for this process too
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"


# HELPER: Check verbosity
def is_verbose_mode():
    return (
        "--verbose" in sys.argv
        or "-v" in sys.argv
        or os.environ.get("OMNIPKG_VERBOSE") == "1"
    )


# ASCII art madness
CHAOS_HEADER = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—     â•‘
â•‘  â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•    â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘     â•‘
â•‘  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—       â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘     â•‘
â•‘  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘       â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘     â•‘
â•‘  â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘       â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘     â•‘
â•‘   â•šâ•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•       â•šâ•â•   â•šâ•â•  â•šâ•â•     â•‘
â•‘                                                                       â•‘
â•‘              ğŸŒ€ O M N I P K G   C H A O S   T H E O R Y ğŸŒ€           â•‘
â•‘                                                                       â•‘
â•‘        "If it doesn't crash, it wasn't chaotic enough"               â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""


def print_chaos_header():
    print("\033[95m" + CHAOS_HEADER + "\033[0m")
    safe_print("\nğŸ”¥ Initializing Chaos Engine...\n")
    time.sleep(0.5)


# Note: Local definitions of PersistentWorker, TrueSwitcher, and run_in_subprocess
# have been removed in favor of the imported versions from omnipkg.isolation!


def chaos_test_1_version_tornado():
    """ğŸŒªï¸ TEST 1: VERSION TORNADO - Compare Legacy vs Daemon"""
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 1: ğŸŒªï¸  VERSION TORNADO                                â•‘")
    safe_print("â•‘  Benchmark: Legacy Loader vs Daemon Mode                     â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    versions = ["1.24.3", "1.26.4", "2.3.5"]

    # ==================================================================
    # PHASE 1: Legacy omnipkgLoader (Current Implementation)
    # ==================================================================
    safe_print("   ğŸ“ PHASE 1: Legacy omnipkgLoader")
    safe_print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")

    legacy_times = []
    legacy_success = 0

    for i in range(10):  # 10 random switches
        ver = random.choice(versions)
        direction = random.choice(["â†—ï¸", "â†˜ï¸", "â†”ï¸", "â†•ï¸"])

        try:
            start = time.perf_counter()
            with omnipkgLoader(f"numpy=={ver}"):
                import numpy as np

                arr = np.random.rand(50, 50)
                result = np.sum(arr)
                elapsed = (time.perf_counter() - start) * 1000

            legacy_times.append(elapsed)
            legacy_success += 1
            safe_print(
                f"   {direction} Legacy #{i+1:02d}: numpy {ver} â†’ sum={result:.2f} ({elapsed:.2f}ms)"
            )

        except Exception as e:
            safe_print(f"   ğŸ’¥ Legacy #{i+1:02d}: numpy {ver} â†’ FAILED: {str(e)[:50]}")

        time.sleep(0.02)

    # ==================================================================
    # PHASE 2: Daemon Mode (Using your imports from test 5)
    # ==================================================================
    safe_print("\n   ğŸ“ PHASE 2: Daemon Mode")
    safe_print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")

    daemon_times = []
    daemon_success = 0

    try:
        # Same imports as test 5
        from omnipkg.isolation.worker_daemon import DaemonClient, DaemonProxy

        safe_print("   âš¡ Initializing DaemonClient...")
        daemon_init_start = time.perf_counter()
        client = DaemonClient()

        # Verify daemon is up or start it
        status = client.status()
        if not status.get("success"):
            safe_print("   âš¡ Starting daemon...")
            from omnipkg.isolation.worker_daemon import WorkerPoolDaemon

            WorkerPoolDaemon().start(daemonize=True)
            time.sleep(1)

        daemon_init_time = (time.perf_counter() - daemon_init_start) * 1000
        safe_print(f"   âš¡ Daemon ready in {daemon_init_time:.2f}ms\n")

        for i in range(10):  # Same 10 random switches
            ver = random.choice(versions)
            direction = random.choice(["â†—ï¸", "â†˜ï¸", "â†”ï¸", "â†•ï¸"])

            try:
                start = time.perf_counter()

                # Use DaemonProxy like test 5
                proxy = DaemonProxy(client, f"numpy=={ver}")

                # Execute numpy code
                code = """
import numpy as np
arr = np.random.rand(50, 50)
result = np.sum(arr)
print(f"{np.__version__}|{result}")
"""
                result = proxy.execute(code)
                elapsed = (time.perf_counter() - start) * 1000

                if result["success"]:
                    output = result["stdout"].strip()
                    if "|" in output:
                        actual_ver, sum_str = output.split("|")
                        daemon_times.append(elapsed)
                        daemon_success += 1
                        safe_print(
                            f"   {direction} Daemon #{i+1:02d}: numpy {ver} â†’ sum={sum_str} ({elapsed:.2f}ms)"
                        )
                    else:
                        safe_print(f"   ğŸ’¥ Daemon #{i+1:02d}: Bad output: {output}")
                else:
                    safe_print(
                        f"   ğŸ’¥ Daemon #{i+1:02d}: Execution failed: {result.get('error', 'Unknown')}"
                    )

            except Exception as e:
                safe_print(f"   ğŸ’¥ Daemon #{i+1:02d}: Exception: {str(e)[:50]}")

            time.sleep(0.02)

    except ImportError as e:
        safe_print(f"   âŒ Daemon mode not available: {e}")
    except Exception as e:
        safe_print(f"   âŒ Daemon error: {str(e)[:50]}")

    # ==================================================================
    # COMPARISON RESULTS
    # ==================================================================
    safe_print("\n   ğŸ“Š COMPARISON RESULTS")
    safe_print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")

    # Legacy Results
    if legacy_times:
        avg_legacy = sum(legacy_times) / len(legacy_times)
        safe_print("   ğŸ§“ Legacy omnipkgLoader:")
        safe_print(f"      Success: {legacy_success}/10")
        safe_print(f"      Avg Time: {avg_legacy:.2f}ms per switch")
        safe_print(f"      Total: {sum(legacy_times):.2f}ms")
    else:
        safe_print("   ğŸ§“ Legacy omnipkgLoader: FAILED")

    safe_print("")

    # Daemon Results
    if daemon_times:
        avg_daemon = sum(daemon_times) / len(daemon_times)
        safe_print("   âš¡ Daemon Mode:")
        safe_print(f"      Success: {daemon_success}/10")
        safe_print(f"      Avg Time: {avg_daemon:.2f}ms per switch")
        safe_print(f"      Total: {sum(daemon_times):.2f}ms")

        # Calculate speedup
        if legacy_times:
            speedup = avg_legacy / avg_daemon if avg_daemon > 0 else float("inf")
            safe_print(f"      ğŸš€ Speedup: {speedup:.1f}x faster!")
    else:
        safe_print("   âš¡ Daemon Mode: NOT AVAILABLE")

    # Overall verdict
    safe_print("\n")
    if legacy_success >= 8 and daemon_success >= 8:
        safe_print("âœ… TORNADO SURVIVED IN BOTH MODES!")
        return True
    elif legacy_success >= 8:
        safe_print("âœ… TORNADO SURVIVED (Legacy Mode)")
        return True
    elif daemon_success >= 8:
        safe_print("âœ… TORNADO SURVIVED (Daemon Mode)")
        return True
    else:
        safe_print("âš¡ TORNADO PARTIALLY SURVIVED")
        return legacy_success > 0 or daemon_success > 0


def chaos_test_2_dependency_inception():
    """ğŸ­ TEST 2: DEPENDENCY INCEPTION - BENCHMARK EDITION"""
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 2: ğŸ­  DEPENDENCY INCEPTION (BENCHMARK)               â•‘")
    safe_print("â•‘  10 Levels Deep. Comparison: Local Stack vs Daemon Stack     â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    versions = ["1.24.3", "1.26.4", "2.3.5"]
    MAX_DEPTH = 10
    verbose = is_verbose_mode()  # <--- CHECK VERBOSITY

    # ==================================================================
    # PHASE 1: LEGACY MODE (Local Recursion)
    # ==================================================================
    safe_print("   ğŸ“ PHASE 1: Legacy omnipkgLoader (Local Process)")
    safe_print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    depth_legacy = 0
    start_legacy = time.perf_counter()

    def go_deeper_legacy(level):
        nonlocal depth_legacy
        if level > MAX_DEPTH:
            return

        indent = "  " * level
        ver = random.choice(versions)

        safe_print(f"   {indent}{'ğŸ”»' * level} Level {level}: numpy {ver}")

        # We MUST do the import INSIDE the with block
        with omnipkgLoader(f"numpy=={ver}", quiet=not verbose, worker_fallback=True):
            import numpy as np

            # Now we check the version
            if np.__version__ != ver:
                safe_print(f"   ğŸ’¥ Mismatch! Expected {ver} got {np.__version__}")

            depth_legacy = max(depth_legacy, level)

            if level < MAX_DEPTH:
                go_deeper_legacy(level + 1)
            else:
                safe_print(f"   {indent}{'ğŸ’¥' * 10} REACHED THE CORE!")

    try:
        go_deeper_legacy(1)
    except Exception as e:
        safe_print(f"   âŒ Legacy Phase Failed: {e}")

    total_legacy_time = time.perf_counter() - start_legacy
    safe_print(f"\n   â±ï¸  Legacy Time: {total_legacy_time:.3f}s")

    # ==================================================================
    # PHASE 2: DAEMON MODE (Remote Recursion)
    # ==================================================================
    safe_print("\n   ğŸ“ PHASE 2: Daemon Mode (Remote Execution)")
    safe_print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    safe_print("   ğŸ”¥ Sending recursive payload to Daemon Worker...")

    # Initialize Client
    try:
        from omnipkg.isolation.worker_daemon import (
            DaemonClient,
            DaemonProxy,
            WorkerPoolDaemon,
        )

        client = DaemonClient()
        if not client.status().get("success"):
            safe_print("   âš™ï¸  Starting Daemon...")
            WorkerPoolDaemon().start(daemonize=True)
            time.sleep(2)
    except ImportError:
        safe_print("   âŒ Daemon modules missing.")
        return False

    start_daemon = time.perf_counter()

    # We pass the 'verbose' flag into the remote script too!
    remote_code = f"""
import sys
import random
from omnipkg.loader import omnipkgLoader

depth = 0
MAX_DEPTH = {MAX_DEPTH}
versions = {versions}
IS_VERBOSE = {str(verbose)}

def log(msg):
    sys.stdout.write(msg + '\\n')

def go_deeper(level):
    global depth
    if level > MAX_DEPTH: return
    
    ver = random.choice(versions)
    
    # Send specific marker for the test runner to parse
    log(f"L{{level}}|{{ver}}")
    
    # Pass verbosity to the remote loader
    with omnipkgLoader(f"numpy=={{ver}}", quiet=not IS_VERBOSE, isolation_mode='overlay'):
        import numpy as np
        depth = max(depth, level)
        if level < MAX_DEPTH:
            go_deeper(level + 1)
        else:
            log("CORE_REACHED")

go_deeper(1)
"""
    try:
        # Use a base worker to execute the complex logic
        proxy = DaemonProxy(client, "numpy==1.26.4")
        result = proxy.execute(remote_code)

        total_daemon_time = time.perf_counter() - start_daemon

        if result["success"]:
            # Visualize the remote output instantly
            # If verbose, we also print the loader logs captured in stdout
            lines = result["stdout"].strip().split("\n")
            for line in lines:
                if "|" in line and line.startswith("L"):
                    parts = line.split("|")
                    lvl = int(parts[0][1:])
                    ver = parts[1]
                    indent = "  " * lvl
                    safe_print(f"   âš¡ {indent}{'âš¡' * lvl} Level {lvl}: numpy {ver}")
                elif "CORE_REACHED" in line:
                    indent = "  " * MAX_DEPTH
                    safe_print(
                        f"   âš¡ {indent}{'ğŸ’¥' * 10} REACHED THE CORE (REMOTELY)!"
                    )
                elif verbose:
                    # Print raw loader logs if verbose is ON
                    safe_print(f"      [Remote] {line}")
        else:
            safe_print(f"   âŒ Daemon Execution Failed: {result['error']}")
            total_daemon_time = float("inf")

    except Exception as e:
        safe_print(f"   âŒ Daemon Error: {e}")
        total_daemon_time = float("inf")

    # ==================================================================
    # FINAL SCOREBOARD
    # ==================================================================
    safe_print(f"\n{'='*60}")
    safe_print(f"ğŸ“Š INCEPTION RESULTS ({MAX_DEPTH} Nested Layers)")
    safe_print(f"{'='*60}")

    safe_print(f"{'METRIC':<20} | {'LEGACY':<15} | {'DAEMON':<15}")
    safe_print("-" * 60)
    safe_print(
        f"{'Total Time':<20} | {total_legacy_time:.3f}s          | {total_daemon_time:.3f}s"
    )

    if total_daemon_time < total_legacy_time:
        speedup = total_legacy_time / total_daemon_time
        safe_print("-" * 60)
        safe_print(f"ğŸš€ SPEEDUP FACTOR: {speedup:.1f}x FASTER")

    if total_daemon_time < float("inf"):
        safe_print("\nâœ… WE WENT DEEPER (IN BOTH DIMENSIONS)!")
        return True
    else:
        safe_print("\nâš ï¸  DAEMON STACK FAILED")
        return False


def chaos_test_3_framework_battle_royale():
    """âš”ï¸ TEST 3: FRAMEWORK BATTLE ROYALE (DAEMON EDITION)"""
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 3: âš”ï¸  FRAMEWORK BATTLE ROYALE (TRULY CONCURRENT)    â•‘")
    safe_print("â•‘  All 4 frameworks executing AT THE SAME EXACT TIME          â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    # 1. Connect to Daemon and measure startup
    try:
        from omnipkg.isolation.worker_daemon import DaemonClient, WorkerPoolDaemon
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import numpy as np

        daemon_start = time.perf_counter()
        client = DaemonClient()
        status = client.status()
        daemon_connect_time = (time.perf_counter() - daemon_start) * 1000

        if not status.get("success"):
            safe_print("   âš™ï¸  Summoning the Arena (Daemon)...")
            vip_specs = [
                "tensorflow==2.13.0",
                "torch==2.0.1+cu118",
                "numpy==1.24.3",
                "numpy==2.3.5",
            ]
            WorkerPoolDaemon(warmup_specs=vip_specs).start(daemonize=True)
            time.sleep(3)
            daemon_start = time.perf_counter()
            client = DaemonClient()
            daemon_connect_time = (time.perf_counter() - daemon_start) * 1000

        safe_print(f"âš¡ Daemon connection established in {daemon_connect_time:.2f}ms\n")

    except ImportError:
        return False

    # 2. Define The Fighters
    combatants = [
        (
            "TensorFlow",
            "tensorflow==2.13.0",
            "import tensorflow as tf; result = {'output': f'TensorFlow {tf.__version__} | Sum: {tf.reduce_sum(tf.constant([1, 2, 3])).numpy()}'}",
        ),
        (
            "PyTorch",
            "torch==2.0.1+cu118",
            "import torch; result = {'output': f'PyTorch {torch.__version__} | Sum: {torch.sum(torch.tensor([1, 2, 3])).item()}'}",
        ),
        (
            "NumPy Legacy",
            "numpy==1.24.3",
            "import numpy as np; result = {'output': f'NumPy {np.__version__} | Sum: {np.sum(np.array([1, 2, 3]))}'}",
        ),
        (
            "NumPy Modern",
            "numpy==2.3.5",
            "import numpy as np; result = {'output': f'NumPy {np.__version__} | Sum: {np.sum(np.array([1, 2, 3]))}'}",
        ),
    ]

    def execute_fighter(name, spec, code):
        t_start = time.perf_counter()
        res = client.execute_smart(spec, code)
        duration = (time.perf_counter() - t_start) * 1000
        return (name, res, duration)

    safe_print("ğŸ¥Š ROUND 1: Truly Concurrent Execution\n")

    wall_clock_start = time.perf_counter()

    # Execute all 4 in parallel threads
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [
            executor.submit(execute_fighter, name, spec, code)
            for name, spec, code in combatants
        ]

        results = []
        for future in as_completed(futures):
            results.append(future.result())

    wall_clock_total = (time.perf_counter() - wall_clock_start) * 1000

    # Sort by name for consistent display
    results.sort(key=lambda x: x[0])

    total_individual_time = 0
    for name, res, duration in results:
        total_individual_time += duration
        if res.get("success"):
            # The result dict contains the 'output' key from worker execution
            if isinstance(res.get("result"), dict) and "output" in res["result"]:
                output = res["result"]["output"]
            else:
                # Fallback to stdout or stringified result
                output = res.get("meta", {}).get("stdout", "").strip()
                if not output:
                    output = str(res.get("result", "")).strip()

            safe_print(f"   âš¡ {name:<15} â†’ {output} ({duration:.2f}ms)")
        else:
            safe_print(f"   ğŸ’¥ {name:<15} â†’ FAILED ({duration:.2f}ms)")

    safe_print("\nğŸ“Š CONCURRENCY RESULTS:")
    safe_print(f"   Wall Clock Time: {wall_clock_total:.2f}ms")
    safe_print(f"   Sum of Individual Times: {total_individual_time:.2f}ms")
    safe_print(
        f"   ğŸš€ Parallelism Factor: {total_individual_time/wall_clock_total:.1f}x"
    )
    safe_print(f"   âš¡ Daemon Overhead: {daemon_connect_time:.2f}ms\n")

    """âš”ï¸ TEST 3: FRAMEWORK BATTLE ROYALE (DAEMON EDITION)"""
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 3: âš”ï¸  FRAMEWORK BATTLE ROYALE (DAEMON)               â•‘")
    safe_print("â•‘  TensorFlow, PyTorch, JAX, NumPy - ALL IN MEMORY AT ONCE     â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    # 1. Connect to Daemon
    try:
        from omnipkg.isolation.worker_daemon import DaemonClient, WorkerPoolDaemon

        client = DaemonClient()
        if not client.status().get("success"):
            safe_print("   âš™ï¸  Summoning the Arena (Daemon)...")
            # Pre-warm the combatants
            vip_specs = [
                "tensorflow==2.13.0",
                "torch==2.0.1+cu118",
                "numpy==1.24.3",
                "numpy==2.3.5",
            ]
            WorkerPoolDaemon(warmup_specs=vip_specs).start(daemonize=True)
            time.sleep(3)  # Give TF time to boot (it's heavy)

    except ImportError:
        return False

    # 2. Define The Fighters
    combatants = [
        {
            "name": "TensorFlow",
            "spec": "tensorflow==2.13.0",
            "code": "import tensorflow as tf; print(f'TensorFlow {tf.__version__} | Sum: {tf.reduce_sum(tf.constant([1, 2, 3])).numpy()}')",
        },
        {
            "name": "PyTorch",
            "spec": "torch==2.0.1+cu118",
            "code": "import torch; print(f'PyTorch {torch.__version__}    | Sum: {torch.sum(torch.tensor([1, 2, 3])).item()}')",
        },
        {
            "name": "NumPy Legacy",
            "spec": "numpy==1.24.3",
            "code": "import numpy as np; print(f'NumPy {np.__version__}      | Sum: {np.sum(np.array([1, 2, 3]))}')",
        },
        {
            "name": "NumPy Modern",
            "spec": "numpy==2.3.5",
            "code": "import numpy as np; print(f'NumPy {np.__version__}      | Sum: {np.sum(np.array([1, 2, 3]))}')",
        },
    ]

    safe_print("ğŸ¥Š ROUND 2: Simultaneous Execution via Daemon\n")

    total_start = time.perf_counter()

    # We will launch them sequentially to see the latency,
    # but the daemon keeps them all resident in RAM.

    for fighter in combatants:
        t_start = time.perf_counter()

        # Smart Execute (Data is None, so it uses JSON path automatically)
        res = client.execute_smart(fighter["spec"], fighter["code"])

        duration = (time.perf_counter() - t_start) * 1000

        if res.get("success"):
            output = res["result"].strip()
            # Clean up the output string for display
            clean_out = output.split("\n")[-1] if "\n" in output else output
            safe_print(f"   âš¡ {fighter['name']:<15} â†’ {clean_out} ({duration:.2f}ms)")
        else:
            safe_print(f"   ğŸ’¥ {fighter['name']:<15} â†’ FAILED: {res.get('error')[:50]}")

    safe_print(
        f"\nâœ… ALL COMBATANTS TESTED in {(time.perf_counter() - total_start):.2f}s!\n"
    )

    # ---------------------------------------------------------
    # ROUND 2: The "Smart" Data Hand-off
    # ---------------------------------------------------------
    safe_print("ğŸ¥Š ROUND 3: Smart Data Hand-off (1MB Array)\n")

    import numpy as np

    data = np.ones(1024 * 128)  # 1MB of floats (128K * 8 bytes)

    # TF Sum via Smart Client
    t_start = time.perf_counter()

    # We pass the data. execute_smart will see 1MB > 64KB and choose SHM.
    # Code assumes 'arr_in' exists and writes to 'arr_out'
    tf_shm_code = """
import tensorflow as tf
# Convert SHM input (numpy) to Tensor, sum it, write back to SHM output
# Note: For scalar output in SHM, we write to index 0
val = tf.reduce_sum(tf.constant(arr_in))
arr_out[0] = val.numpy()
"""

    res = client.execute_smart("tensorflow==2.13.0", tf_shm_code, data=data)

    duration = (time.perf_counter() - t_start) * 1000

    if res.get("success"):
        result_val = res["result"][0]
        transport = res.get("transport", "UNKNOWN")
        safe_print(
            f"   ğŸš€ TF 2.13 (1MB)   â†’ Sum: {result_val:.0f} via {transport} ({duration:.2f}ms)"
        )
    else:
        safe_print(f"   ğŸ’¥ TF Failed: {res.get('error')}")

    return True
    # ---------------------------------------------------------
    # ROUND 2: Zero-Copy Shared Memory Data Hand-off
    # ---------------------------------------------------------
    safe_print("ğŸ¥Š ROUND 3: Zero-Copy Shared Memory (1MB Array)\n")

    # 1MB of floats (128K * 8 bytes)
    data = np.ones(1024 * 128, dtype=np.float64)
    data_size_mb = data.nbytes / (1024 * 1024)

    safe_print(f"   ğŸ“¦ Input: {data_size_mb:.2f}MB array ({len(data):,} elements)\n")

    # TensorFlow Sum via SHM
    t_start = time.perf_counter()

    tf_shm_code = """
import tensorflow as tf
# arr_in and arr_out are already mapped via shared memory
val = tf.reduce_sum(tf.constant(arr_in))
arr_out[0] = val.numpy()
result = {'sum': float(arr_out[0])}
"""

    res = client.execute_smart("tensorflow==2.13.0", tf_shm_code, data=data)
    tf_duration = (time.perf_counter() - t_start) * 1000

    if res.get("success"):
        result_val = (
            res["result"][0] if isinstance(res["result"], np.ndarray) else res["result"]
        )
        transport = res.get("transport", "UNKNOWN")
        safe_print(
            f"   ğŸš€ TensorFlow 2.13 â†’ Sum: {result_val:.0f} via {transport} ({tf_duration:.2f}ms)"
        )
    else:
        safe_print(f"   ğŸ’¥ TensorFlow Failed: {res.get('error')}")

    # PyTorch Sum via SHM
    t_start = time.perf_counter()

    torch_shm_code = """
import torch
val = torch.sum(torch.from_numpy(arr_in))
arr_out[0] = val.item()
result = {'sum': float(arr_out[0])}
"""

    res = client.execute_smart("torch==2.0.1+cu118", torch_shm_code, data=data)
    torch_duration = (time.perf_counter() - t_start) * 1000

    if res.get("success"):
        result_val = (
            res["result"][0] if isinstance(res["result"], np.ndarray) else res["result"]
        )
        transport = res.get("transport", "UNKNOWN")
        safe_print(
            f"   ğŸš€ PyTorch 2.0     â†’ Sum: {result_val:.0f} via {transport} ({torch_duration:.2f}ms)"
        )
    else:
        safe_print(f"   ğŸ’¥ PyTorch Failed: {res.get('error')}")

    # NumPy Sum via SHM
    t_start = time.perf_counter()

    numpy_shm_code = """
import numpy as np
val = np.sum(arr_in)
arr_out[0] = val
result = {'sum': float(arr_out[0])}
"""

    res = client.execute_smart("numpy==2.3.5", numpy_shm_code, data=data)
    numpy_duration = (time.perf_counter() - t_start) * 1000

    if res.get("success"):
        result_val = (
            res["result"][0] if isinstance(res["result"], np.ndarray) else res["result"]
        )
        transport = res.get("transport", "UNKNOWN")
        safe_print(
            f"   ğŸš€ NumPy 2.3.5     â†’ Sum: {result_val:.0f} via {transport} ({numpy_duration:.2f}ms)"
        )
    else:
        safe_print(f"   ğŸ’¥ NumPy Failed: {res.get('error')}")

    safe_print("\nğŸ“Š ZERO-COPY PERFORMANCE:")
    safe_print(f"   Data Size: {data_size_mb:.2f}MB")
    safe_print(
        f"   TensorFlow: {tf_duration:.2f}ms ({data_size_mb/tf_duration*1000:.0f} MB/s)"
    )
    safe_print(
        f"   PyTorch:    {torch_duration:.2f}ms ({data_size_mb/torch_duration*1000:.0f} MB/s)"
    )
    safe_print(
        f"   NumPy:      {numpy_duration:.2f}ms ({data_size_mb/numpy_duration*1000:.0f} MB/s)"
    )
    safe_print("   ğŸ¯ Zero-copy means NO data serialization overhead!\n")

    return True


def chaos_test_4_memory_madness():
    """ğŸ§  TEST 4: MEMORY MADNESS - Allocate everywhere"""
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 4: ğŸ§   MEMORY MADNESS                                  â•‘")
    safe_print("â•‘  Simultaneous memory allocation across version boundaries    â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    allocations = []
    versions = ["1.24.3", "1.26.4", "2.3.5"]

    for i, ver in enumerate(versions):
        with omnipkgLoader(f"numpy=={ver}"):
            import numpy as np

            # Allocate increasingly large arrays
            size = (1000 * (i + 1), 1000 * (i + 1))
            arr = np.ones(size)
            mem_mb = arr.nbytes / 1024 / 1024
            addr = hex(id(arr))

            allocations.append((ver, mem_mb, addr))
            safe_print(f"ğŸ§  numpy {ver}: Allocated {mem_mb:.1f}MB at {addr}")

    safe_print(f"\nğŸ¯ Total allocations: {len(allocations)}")
    safe_print(f"ğŸ¯ Unique memory addresses: {len(set(a[2] for a in allocations))}")
    safe_print("âœ… MEMORY CHAOS CONTAINED!\n")


def chaos_test_5_race_condition_roulette():
    """ğŸ° TEST 5: RACE CONDITION ROULETTE - ZERO-COPY SHM EDITION"""
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 5: ğŸ°  RACE CONDITION ROULETTE (SHM TURBO)            â•‘")
    safe_print("â•‘  10 Threads x 3 Swaps. 100% Zero-Copy Data Transfer.         â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    results = {}
    versions = ["numpy==1.24.3", "numpy==1.26.4", "numpy==2.3.5"]
    print_lock = threading.Lock()
    verbose = is_verbose_mode()

    # Initialize Client
    try:
        import numpy as np
        from omnipkg.isolation.worker_daemon import DaemonClient, WorkerPoolDaemon

        client = DaemonClient()
        if not client.status().get("success"):
            safe_print("   âŒ Daemon not running! Starting...")
            WorkerPoolDaemon().start(daemonize=True)
            time.sleep(2)
    except ImportError:
        return False

    def chaotic_worker(thread_id):
        thread_versions = [random.choice(versions) for _ in range(3)]
        thread_results = []

        for i, spec in enumerate(thread_versions):
            # 1. Generate Data Locally
            local_data = np.random.rand(50, 50)

            # 2. Worker Code: Read SHM, Compute Det, Write SHM, Print Version
            code = """
import numpy as np
# arr_in provided via SHM
det = np.linalg.det(arr_in)
# arr_out provided via SHM (size 1 array)
arr_out[0] = det
# Print version to verify environment
print(np.__version__)
"""
            t_start = time.perf_counter()
            try:
                # 3. Execute via Zero-Copy
                # Output shape is (1,) because determinant is a scalar
                result_arr, response = client.execute_zero_copy(
                    spec,
                    code,
                    input_array=local_data,
                    output_shape=(1,),
                    output_dtype="float64",
                )

                t_end = time.perf_counter()
                duration_ms = (t_end - t_start) * 1000

                # 4. Verify Data (Local vs Remote)
                local_det = np.linalg.det(local_data)
                remote_det = result_arr[0]

                # 5. Verify Version (from stdout)
                remote_version = response["stdout"].strip()

                if np.isclose(local_det, remote_det):
                    status = "âœ…"
                    msg = f"{remote_version:<14}"
                else:
                    status = "âŒ"
                    msg = f"MATH ERROR: {local_det} vs {remote_det}"

                thread_results.append((spec, remote_version, status))

                if verbose:
                    with print_lock:
                        safe_print(
                            f"   ğŸ² Thread {thread_id:02d} Round {i+1}: {msg} â†’ {duration_ms:>6.2f} ms"
                        )

            except Exception as e:
                thread_results.append((spec, str(e), "âŒ"))
                with print_lock:
                    safe_print(f"   ğŸ’¥ Thread {thread_id:02d}: {e}")

        results[thread_id] = thread_results

    safe_print("ğŸ”¥ Launching 10 concurrent threads hammering SHM subsystem...")

    race_start = time.perf_counter()
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(chaotic_worker, i) for i in range(10)]
        for f in futures:
            f.result()
    race_time = time.perf_counter() - race_start

    total_switches = 0
    successful_switches = 0

    for thread_id, thread_results in results.items():
        total_switches += len(thread_results)
        successful_switches += sum(1 for r in thread_results if r[2] == "âœ…")

    safe_print(f"\n{'='*60}")
    safe_print(f"ğŸ¯ Total Requests: {total_switches}")
    safe_print(
        f"âœ… Success Rate:   {successful_switches}/{total_switches} ({successful_switches/total_switches*100:.1f}%)"
    )
    safe_print(f"âš¡ Total Time:     {race_time:.3f}s")
    safe_print(f"âš¡ Throughput:     {total_switches/race_time:.1f} swaps/sec")
    safe_print(f"ğŸš€ Avg Latency:    {(race_time/total_switches)*1000:.1f} ms/swap")

    if successful_switches == total_switches:
        safe_print("âœ… CHAOS SURVIVED! (Memory Integrity Verified)")
    else:
        safe_print("âš ï¸  PARTIAL FAILURE")
    print("=" * 60 + "\n")

    return successful_switches == total_switches


def chaos_test_6_version_time_machine():
    """â° TEST 6: VERSION TIME MACHINE - Past, present, future"""
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 6: â°  VERSION TIME MACHINE                           â•‘")
    safe_print("â•‘  Travel through NumPy history at light speed                 â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    timeline = [
        ("ğŸ¦• PREHISTORIC", "numpy==1.23.5", "2022"),
        ("ğŸ›ï¸  ANCIENT", "numpy==1.24.3", "2023"),
        ("ğŸ“œ LEGACY", "numpy==1.26.4", "2024"),
        ("ğŸš€ MODERN", "numpy==2.3.5", "2024"),
    ]

    print("â° Initiating temporal displacement...\n")

    for era, spec, year in timeline:
        try:
            safe_print(f"ğŸŒ€ Jumping to {year}...")
            with omnipkgLoader(spec):
                import numpy as np

                arr = np.array([1, 2, 3, 4, 5])
                mean = arr.mean()
                print(f"   {era:20} {spec:20} â†’ mean={mean}")
        except Exception as e:
            safe_print(f"   âš ï¸  {era}: Time jump failed - {e}")
        time.sleep(0.2)

    safe_print("\nâœ… TIME TRAVEL COMPLETE!\n")


def chaos_test_7_dependency_jenga():
    """ğŸ² TEST 7: DEPENDENCY JENGA - Remove pieces carefully"""
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 7: ğŸ²  DEPENDENCY JENGA                               â•‘")
    safe_print("â•‘  Stack versions carefully... DON'T LET IT FALL!              â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    stack = []
    versions = ["1.24.3", "1.26.4", "2.3.5", "1.26.4", "1.24.3"]

    safe_print("ğŸ² Building the tower...\n")

    for i, ver in enumerate(versions):
        try:
            with omnipkgLoader(f"numpy=={ver}"):
                import numpy as np

                arr = np.random.rand(50, 50)
                checksum = np.sum(arr)
                stack.append((ver, checksum))

                blocks = "ğŸŸ¦" * (i + 1)
                print(
                    f"   {blocks} Level {i+1}: numpy {ver} (checksum: {checksum:.2f})"
                )
                time.sleep(0.1)
        except Exception:
            safe_print(f"   ğŸ’¥ TOWER COLLAPSED AT LEVEL {i+1}!")
            break

    if len(stack) == len(versions):
        safe_print(f"\nğŸ† PERFECT TOWER! All {len(stack)} blocks stable!")
    print()


def chaos_test_8_quantum_superposition():
    """âš›ï¸ TEST 8: QUANTUM SUPERPOSITION - Multiple states at once"""
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 8: âš›ï¸  QUANTUM SUPERPOSITION                          â•‘")
    safe_print("â•‘  Exist in multiple version states simultaneously             â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    safe_print("ğŸŒ€ Entering quantum state...\n")

    states = []

    with omnipkgLoader("numpy==1.24.3"):
        import numpy as np1

        state1 = np1.array([1, 2, 3])
        states.append(("1.24.3", hex(id(state1))))
        safe_print(f"   |Ïˆâ‚âŸ© numpy 1.24.3 exists at {hex(id(state1))}")

        with omnipkgLoader("numpy==1.26.4"):
            import numpy as np2

            state2 = np2.array([4, 5, 6])
            states.append(("1.26.4", hex(id(state2))))
            safe_print(f"   |Ïˆâ‚‚âŸ© numpy 1.26.4 exists at {hex(id(state2))}")

            with omnipkgLoader("numpy==2.3.5"):
                import numpy as np3

                state3 = np3.array([7, 8, 9])
                states.append(("2.3.5", hex(id(state3))))
                safe_print(f"   |Ïˆâ‚ƒâŸ© numpy 2.3.5 exists at {hex(id(state3))}")

                safe_print("\n   ğŸ’« QUANTUM SUPERPOSITION ACHIEVED!")
                safe_print(f"   ğŸ’« {len(states)} states exist simultaneously!")

    safe_print("\nâœ… WAVE FUNCTION COLLAPSED SAFELY!\n")


def chaos_test_9_import_hell():
    """ğŸ”¥ TEST 9: IMPORT HELL - Conflicting imports everywhere"""
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 9: ğŸ”¥  IMPORT HELL                                    â•‘")
    safe_print("â•‘  Import conflicts that should destroy Python itself          â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    safe_print("ğŸ”¥ Descending into import hell...\n")

    # --- MODIFIED: Use PersistentWorker for TensorFlow ---
    safe_print("   ğŸ˜ˆ Circle 1: TensorFlow Reality (Persistent Worker)")

    verbose = is_verbose_mode()  # <--- Check verbosity
    tf_worker = PersistentWorker(
        "tensorflow==2.13.0", verbose=verbose
    )  # <--- Pass it here
    try:
        tf_code = """
from omnipkg.loader import omnipkgLoader
import sys

with omnipkgLoader("numpy==1.23.5"):
    import tensorflow as tf
    x = tf.constant([1, 2, 3])
    result = tf.reduce_sum(x).numpy()
    sys.stderr.write(f"      ğŸ”¥ TensorFlow {tf.__version__} + NumPy survival: sum={result}\\n")
"""
        result = tf_worker.execute(tf_code)
        if result["success"]:
            safe_print("      âœ… TensorFlow Reality survived")
        else:
            safe_print(f"      âš ï¸  TensorFlow failed: {result['error'][:60]}...")
    finally:
        tf_worker.shutdown()

    # Circle 2: NumPy Standalone
    safe_print("   ğŸ˜ˆ Circle 2: NumPy Standalone")
    try:
        with omnipkgLoader("numpy==1.24.3"):
            import numpy as np

            safe_print(f"      âœ… numpy {np.__version__} survived")
    except Exception as e:
        error_msg = str(e).split("\n")[0][:60]
        safe_print(f"      âš ï¸  numpy==1.24.3 - {error_msg}...")

    # Circle 3: PyTorch Inferno
    safe_print("   ğŸ˜ˆ Circle 3: PyTorch Inferno")
    try:
        with omnipkgLoader("torch==2.0.1+cu118"):
            import torch

            safe_print(f"      âœ… torch {torch.__version__} survived")
    except Exception as e:
        error_msg = str(e).split("\n")[0][:60]
        safe_print(f"      âš ï¸  torch==2.0.1+cu118 - {error_msg}...")

    # Circle 4: NumPy Chaos
    safe_print("   ğŸ˜ˆ Circle 4: NumPy Chaos")
    for numpy_ver in ["1.26.4", "2.3.5", "1.24.3"]:
        try:
            with omnipkgLoader(f"numpy=={numpy_ver}"):
                import numpy as np

                safe_print(f"      âœ… numpy {np.__version__} survived")
        except Exception as e:
            error_msg = str(e).split("\n")[0][:60]
            safe_print(f"      âš ï¸  numpy=={numpy_ver} - {error_msg}...")

    # Circle 5: Mixed Madness
    safe_print("   ğŸ˜ˆ Circle 5: Mixed Madness")
    try:
        with omnipkgLoader("torch==2.0.1+cu118"):
            import torch

            safe_print(f"      âœ… torch {torch.__version__} survived")
    except Exception as e:
        safe_print(f"      âš ï¸  torch - {str(e)[:60]}...")

    try:
        with omnipkgLoader("numpy==2.3.5"):
            import numpy as np

            safe_print(f"      âœ… numpy {np.__version__} survived")
    except Exception as e:
        safe_print(f"      âš ï¸  numpy - {str(e)[:60]}...")

    time.sleep(0.1)
    safe_print("\nâœ… ESCAPED FROM HELL!\n")


def chaos_test_10_grand_finale():
    """ğŸ† TEST 10: GRAND FINALE - Everything at once"""
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 10: ğŸ†  GRAND FINALE                                  â•‘")
    safe_print("â•‘  MAXIMUM CHAOS - ALL TESTS SIMULTANEOUSLY                    â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    safe_print("ğŸ† Initiating maximum chaos sequence...\n")

    from omnipkg.core import ConfigManager

    cm = ConfigManager(suppress_init_messages=True)
    omnipkg_config = cm.config

    def mini_tornado():
        for unused in range(3):
            with omnipkgLoader(
                f"numpy=={random.choice(['1.24.3', '2.3.5'])}", config=omnipkg_config
            ):
                import numpy as np

                np.random.rand(100, 100).sum()

    def mini_inception(level=0):
        if level < 3:
            with omnipkgLoader(
                f"numpy=={random.choice(['1.24.3', '1.26.4'])}", config=omnipkg_config
            ):
                mini_inception(level + 1)

    safe_print("ğŸŒªï¸  Launching chaos tornado...")
    mini_tornado()

    safe_print("ğŸ­ Executing mini inception...")
    mini_inception()

    safe_print("ğŸ§  Rapid memory allocation...")
    for ver in ["1.24.3", "2.3.5"]:
        with omnipkgLoader(f"numpy=={ver}", config=omnipkg_config):
            import numpy as np

            np.ones((500, 500))

    print("â° Time travel sequence...")
    for ver in ["1.24.3", "2.3.5", "1.24.3"]:
        with omnipkgLoader(f"numpy=={ver}", config=omnipkg_config):
            import numpy as np

            pass

    safe_print("\nğŸ†ğŸ†ğŸ† MAXIMUM CHAOS SURVIVED! ğŸ†ğŸ†ğŸ†\n")


def chaos_test_11_tensorflow_resurrection():
    """âš°ï¸ TEST 11: TENSORFLOW RESURRECTION ULTIMATE"""
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 11: âš°ï¸ğŸ’€âš¡ TENSORFLOW RESURRECTION ULTIMATE          â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    verbose = is_verbose_mode()

    # ================================================================
    # PART A: TRUE SEQUENTIAL WORKER SPAWN (Kill after each use)
    # ================================================================
    safe_print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    safe_print("â”‚ PART A: âš¡ TRUE SEQUENTIAL WORKER RESURRECTION               â”‚")
    safe_print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n")

    safe_print("   ğŸ“ Method 1: Sequential Workers (FRESH PROCESS EACH TIME)")
    safe_print("      (Measuring true 'Wall Clock' time from process start to result)")
    sequential_times = []

    for i in range(5):
        safe_print(f"\n      ğŸ”„ Iteration {i+1}/5: Spawning & executing...")

        # CRITICAL: Measure time starting BEFORE process creation
        start_wall = time.perf_counter()

        worker = None
        try:
            # 1. Initialize the heavy process
            worker = PersistentWorker("tensorflow==2.13.0", verbose=verbose)

            # 2. Run the code
            code = """
from omnipkg.loader import omnipkgLoader
import sys
with omnipkgLoader("tensorflow==2.13.0"):
    import tensorflow as tf
    x = tf.constant([1, 2, 3])
    result = tf.reduce_sum(x)
"""
            result = worker.execute(code)

            # 3. Calculate Wall Clock Time
            elapsed = (time.perf_counter() - start_wall) * 1000

            if result.get("success"):
                sequential_times.append(elapsed)
                safe_print(f"         âœ… Full Lifecycle: {elapsed:.0f}ms")
            else:
                safe_print(f"         âŒ Failed: {result.get('error')}")

        except Exception as e:
            safe_print(f"         âŒ Exception: {e}")

        finally:
            if worker:
                safe_print("         ğŸ›‘ Killing worker for fresh restart...")
                try:
                    worker.shutdown()
                except:
                    pass
            time.sleep(0.2)

    avg_sequential = (
        sum(sequential_times) / len(sequential_times) if sequential_times else 0
    )
    safe_print(
        f"\n   ğŸ“Š Sequential Average: {avg_sequential:.0f}ms per TRUE resurrection"
    )

    # ================================================================
    # PART B: DAEMON MODE (Fair Test - Fresh Daemon)
    # ================================================================
    safe_print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    safe_print("â”‚ PART B: âš¡ DAEMON MODE (Persistent Worker - FAIR TEST)      â”‚")
    safe_print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n")

    daemon_times = []
    daemon_available = False

    try:
        from omnipkg.isolation.worker_daemon import DaemonClient, DaemonProxy
        import subprocess

        safe_print("   ğŸ§¹ Restarting Daemon...")
        subprocess.run(
            ["8pkg", "daemon", "stop"],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )
        time.sleep(1)
        subprocess.run(
            ["8pkg", "daemon", "start"],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )

        client = DaemonClient()
        for unused in range(10):
            time.sleep(0.5)
            if client.status().get("success"):
                safe_print("   âœ… Daemon online\n")
                break

        safe_print("   ğŸ“ Running 5 iterations with persistent worker:\n")

        proxy = DaemonProxy(client, "tensorflow==2.13.0")

        for i in range(5):
            safe_print(f"      Iteration {i+1}/5...")
            start = time.perf_counter()

            code = """
from omnipkg.loader import omnipkgLoader
with omnipkgLoader("tensorflow==2.13.0"):
    import tensorflow as tf
    x = tf.constant([1, 2, 3])
    result = tf.reduce_sum(x)
"""
            result = proxy.execute(code)
            elapsed = (time.perf_counter() - start) * 1000

            if result.get("success"):
                daemon_times.append(elapsed)
                safe_print(f"         âš¡ {elapsed:.0f}ms")
            else:
                safe_print("         âŒ Failed")

        avg_daemon = sum(daemon_times) / len(daemon_times) if daemon_times else 0
        speedup = avg_sequential / avg_daemon if avg_daemon > 0 else 0

        safe_print(f"\n   ğŸ“Š Daemon Average: {avg_daemon:.0f}ms")
        safe_print(f"   ğŸš€ SPEEDUP: {speedup:.1f}x faster!\n")
        daemon_available = True

    except Exception as e:
        safe_print(f"   âŒ Daemon mode failed: {e}\n")

    # ================================================================
    # PART C: CONCURRENT SPAWN & HEAVY MATH
    # ================================================================
    safe_print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    safe_print("â”‚ PART C: ğŸ¼ CONCURRENT SPAWN & OPS TEST                      â”‚")
    safe_print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n")

    if not daemon_available:
        return False

    versions = ["2.12.0", "2.13.0", "2.20.0"]

    # ------------------------------------------------------------
    # STEP 1: SEQUENTIAL SPAWN
    # ------------------------------------------------------------
    # We must restart the daemon to ensure no workers are cached
    safe_print("   ğŸ§¹ Restarting Daemon for SEQUENTIAL SPAWN TEST...")
    subprocess.run(
        ["8pkg", "daemon", "stop"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
    )
    time.sleep(1)
    subprocess.run(
        ["8pkg", "daemon", "start"],
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
    )
    time.sleep(2)
    client = DaemonClient()  # Reconnect

    safe_print("   ğŸ¢ STEP 1: Sequential Spawn (One by one)...")
    seq_spawn_start = time.perf_counter()

    for ver in versions:
        t_ver = time.perf_counter()
        safe_print(f"      Requesting TF {ver}...")
        p = DaemonProxy(client, f"tensorflow=={ver}")
        # Execute simple code to force spawn wait
        p.execute(
            f"from omnipkg.loader import omnipkgLoader\nwith omnipkgLoader('tensorflow=={ver}'): import tensorflow as tf"
        )
        safe_print(f"      âœ… Ready in {(time.perf_counter() - t_ver)*1000:.0f}ms")

    seq_spawn_total = (time.perf_counter() - seq_spawn_start) * 1000
    safe_print(f"   â±ï¸  Total Sequential Spawn Time: {seq_spawn_total:.0f}ms\n")

    # ------------------------------------------------------------
    # STEP 2: CONCURRENT SPAWN
    # ------------------------------------------------------------
    # Restart daemon AGAIN to clear cache for fair concurrent test
    safe_print("   ğŸ§¹ Restarting Daemon for CONCURRENT SPAWN TEST...")
    subprocess.run(
        ["8pkg", "daemon", "stop"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
    )
    time.sleep(1)
    subprocess.run(
        ["8pkg", "daemon", "start"],
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
    )
    time.sleep(2)
    client = DaemonClient()  # Reconnect

    safe_print("   ğŸš€ STEP 2: Concurrent Spawn (All at once)...")

    from concurrent.futures import ThreadPoolExecutor

    conc_spawn_start = time.perf_counter()
    active_proxies = {}

    def spawn_worker(ver):
        p = DaemonProxy(client, f"tensorflow=={ver}")
        p.execute(
            f"from omnipkg.loader import omnipkgLoader\nwith omnipkgLoader('tensorflow=={ver}'): import tensorflow as tf"
        )
        return ver, p

    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = [executor.submit(spawn_worker, v) for v in versions]
        for f in futures:
            v, p = f.result()
            active_proxies[v] = p
            safe_print(f"      âœ… Worker Ready: TF {v}")

    conc_spawn_total = (time.perf_counter() - conc_spawn_start) * 1000
    safe_print(f"   â±ï¸  Total Concurrent Spawn Time: {conc_spawn_total:.0f}ms")

    spawn_speedup = seq_spawn_total / conc_spawn_total if conc_spawn_total > 0 else 0
    safe_print(f"   ğŸš€ SPAWN SPEEDUP: {spawn_speedup:.2f}x\n")

    # ------------------------------------------------------------
    # STEP 3: SEQUENTIAL HEAVY OPS
    # ------------------------------------------------------------
    safe_print("   ğŸ¢ STEP 3: Sequential Tensor Operations (Matrix Mult)...")

    heavy_code = """
import tensorflow as tf
import time
size = 2000
with tf.device('/CPU:0'):
    x = tf.random.normal((size, size))
    y = tf.random.normal((size, size))
    z = tf.matmul(x, y)
    _ = z.numpy()
"""

    seq_ops_times = []

    for v in versions:
        safe_print(f"      running on TF {v}...")
        t0 = time.perf_counter()
        res = active_proxies[v].execute(heavy_code)
        dt = (time.perf_counter() - t0) * 1000
        if res.get("success"):
            seq_ops_times.append(dt)
            safe_print(f"         âœ… Done in {dt:.0f}ms")
        else:
            safe_print(f"         âŒ Failed: {res.get('error')}")

    total_seq_ops = sum(seq_ops_times)
    safe_print(f"   ğŸ“Š Total Sequential Calc Time: {total_seq_ops:.0f}ms\n")

    # ------------------------------------------------------------
    # STEP 4: CONCURRENT HEAVY OPS
    # ------------------------------------------------------------
    safe_print("   ğŸš€ STEP 4: Concurrent Tensor Operations...")

    conc_ops_start = time.perf_counter()

    def run_heavy(ver):
        t_start = time.perf_counter()
        active_proxies[ver].execute(heavy_code)
        t_end = time.perf_counter()
        return ver, (t_end - t_start) * 1000

    results_conc = {}
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = [executor.submit(run_heavy, v) for v in versions]
        for f in futures:
            v, dt = f.result()
            results_conc[v] = dt
            safe_print(f"      âœ… TF {v} finished in {dt:.0f}ms")

    total_conc_ops = (time.perf_counter() - conc_ops_start) * 1000

    safe_print("\n   ğŸ“Š Concurrent Calc Summary:")
    safe_print(f"      - Sequential Time: {total_seq_ops:.0f}ms")
    safe_print(f"      - Concurrent Time: {total_conc_ops:.0f}ms")

    if total_conc_ops > 0:
        calc_speedup = total_seq_ops / total_conc_ops
        safe_print(f"      - Calc Speedup: {calc_speedup:.2f}x")

    # ================================================================
    # FINAL RESULTS
    # ================================================================
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  ğŸ“Š FINAL RESULTS                                            â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    safe_print(
        f"   âœ… Resurrection Lag: {avg_sequential:.0f}ms (Cold) vs {avg_daemon:.0f}ms (Warm)"
    )
    safe_print(f"   âœ… Spawning: {seq_spawn_total:.0f}ms -> {conc_spawn_total:.0f}ms")
    safe_print(f"   âœ… Calculation: {total_seq_ops:.0f}ms -> {total_conc_ops:.0f}ms")

    if avg_sequential > 1000:
        safe_print("\n   âœ… TENSORFLOW RESURRECTION: PASSED")
        return True
    else:
        safe_print("\n   âš ï¸  Performance metrics marginal, but functional test PASSED")
        return True


def chaos_test_12_jax_vs_torch_mortal_kombat():
    """ğŸ¥Š TEST 12: TRUE TORCH VERSION SWITCHING - Daemon Edition"""
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 12: ğŸ¥Š TRUE TORCH VERSION SWITCHING (DAEMON)          â•‘")
    safe_print("â•‘  12 Rounds. 2 Fighters. Zero process overhead.              â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    specs = ["torch==2.0.1+cu118", "torch==2.1.0"] * 6  # 12 rounds total

    # 1. Connect to Daemon
    safe_print("âš™ï¸  Connecting to Arena (Daemon)...")
    boot_start = time.perf_counter()

    try:
        from omnipkg.isolation.worker_daemon import (
            DaemonClient,
            DaemonProxy,
            WorkerPoolDaemon,
        )

        client = DaemonClient()

        # Verify daemon is running
        status = client.status()
        if not status.get("success"):
            safe_print("   âš ï¸  Daemon not found. Summoning Daemon...")
            WorkerPoolDaemon().start(daemonize=True)
            time.sleep(1)  # Wait for socket

    except ImportError:
        safe_print("   âŒ Daemon modules missing.")
        return False

    # 2. Initialize Proxies (Lightweight)
    workers = {}
    for spec in ["torch==2.0.1+cu118", "torch==2.1.0"]:
        workers[spec] = DaemonProxy(client, spec)

    boot_time = time.perf_counter() - boot_start
    safe_print(f"âœ¨ Arena Ready in {boot_time*1000:.2f}ms\n")

    successful_rounds = 0
    failed_rounds = 0
    round_times = []

    safe_print("ğŸ”” FIGHT!\n")

    fight_start = time.perf_counter()

    for i, spec in enumerate(specs):
        round_start = time.perf_counter()

        # We pass the round number into the code so the worker prints it
        code_to_run = f"""
import torch
try:
    from .common_utils import safe_print
except ImportError:
    from omnipkg.common_utils import safe_print
# Print directly to stdout, which the daemon captures and returns
x = torch.tensor([1., 2., 3.])
y = torch.sin(x)
print(f"   ğŸ¥Š Round #{i+1}: Fighter {{torch.__version__:<6}} | Hit -> {{y.tolist()}}")
"""
        # Execute via Daemon
        result = workers[spec].execute(code_to_run)

        round_duration = (time.perf_counter() - round_start) * 1000
        round_times.append(round_duration)

        if result["success"]:
            successful_rounds += 1
            # Print the worker's output
            if result.get("stdout"):
                sys.stdout.write(result["stdout"])

            # Print timing overlay
            sys.stdout.write(f"      âš¡ {round_duration:.2f}ms\n")
        else:
            safe_print(f"   ğŸ’¥ FATALITY: {spec} failed - {result.get('error')[:50]}")
            failed_rounds += 1

    total_fight_time = time.perf_counter() - fight_start
    avg_round = sum(round_times) / len(round_times) if round_times else 0

    safe_print(f"\nğŸ¯ Battle Results: {successful_rounds} wins, {failed_rounds} losses")
    safe_print(f"â±ï¸  Total Duration: {total_fight_time:.4f}s")
    safe_print(f"âš¡ Avg Round Time: {avg_round:.2f}ms")

    if successful_rounds == len(specs):
        safe_print("âœ… FLAWLESS VICTORY! (Daemon Handling Perfect Swaps)\n")
    else:
        safe_print("âŒ Some rounds failed.\n")


def chaos_test_13_pytorch_lightning_storm():
    """âš¡ TEST 13: PyTorch Lightning Storm - Using Daemon Workers"""
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 13: âš¡ PyTorch Lightning Storm                         â•‘")
    safe_print("â•‘  Testing framework with daemon-managed workers               â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    safe_print("   ğŸŒ©ï¸  Testing PyTorch Lightning with daemon isolation.\n")

    # Define compatible pairs with their dependencies
    test_configs = [
        {
            "torch": "torch==2.0.1+cu118",
            "lightning": "pytorch-lightning==1.9.0",
            "numpy": "numpy==1.26.4",
            "name": "PyTorch 2.0.1 + Lightning 1.9.0",
        },
        {
            "torch": "torch==2.1.0",
            "lightning": "pytorch-lightning==2.0.0",
            "numpy": "numpy==1.26.4",
            "name": "PyTorch 2.1.0 + Lightning 2.0.0",
        },
    ]

    safe_print("   ğŸŒ©ï¸  Testing PyTorch Lightning with both approaches\n")

    # ==================================================================
    # ROUND 1: Persistent Worker Mode (Traditional)
    # ==================================================================
    safe_print("   ğŸš€ ROUND 1: Persistent Worker Mode")
    safe_print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")

    worker_times = []
    worker_successful = 0

    for i, config in enumerate(test_configs):
        safe_print(f"   ğŸ˜ˆ Test {i+1}/{len(test_configs)}: {config['name']}")

        try:
            # Time the worker boot
            boot_start = time.perf_counter()
            worker = PersistentWorker(config["torch"], verbose=True)
            boot_time = time.perf_counter() - boot_start

            # Execute code
            exec_start = time.perf_counter()
            code_to_run = f"""
from omnipkg.loader import omnipkgLoader

with omnipkgLoader("{config['lightning']}"):
    import pytorch_lightning as pl
    import torch
    import numpy as np
    import sys
    sys.stderr.write(f"      âš¡ PyTorch {{torch.__version__}} + Lightning {{pl.__version__}} + NumPy {{np.__version__}} loaded successfully.\\n")
"""
            result = worker.execute(code_to_run)
            exec_time = time.perf_counter() - exec_start

            total_time = boot_time + exec_time
            worker_times.append(total_time)

            if result["success"]:
                worker_successful += 1
                safe_print(f"      â±ï¸  Boot:     {boot_time*1000:7.2f}ms")
                safe_print(f"      â±ï¸  Execution:{exec_time*1000:7.2f}ms")
                safe_print(f"      â±ï¸  TOTAL:    {total_time*1000:7.2f}ms")
                safe_print(f"      âœ… STRIKE #{worker_successful}!\n")
            else:
                safe_print(f"      ğŸ’¥ Failed: {result['error']}\n")

        except Exception as e:
            safe_print(f"      ğŸ’¥ Exception: {str(e)}\n")
        finally:
            try:
                worker.shutdown()
            except:
                pass

    successful = 0
    verbose = is_verbose_mode()

    # Timing tracking
    total_start = time.perf_counter()
    timing_results = []

    # Initialize daemon client
    try:
        from omnipkg.isolation.worker_daemon import DaemonClient, DaemonProxy

        client = DaemonClient()

        # Verify daemon is running
        status = client.status()
        if not status.get("success"):
            safe_print("   âš™ï¸  Starting daemon...")
            from omnipkg.isolation.worker_daemon import WorkerPoolDaemon

            daemon = WorkerPoolDaemon()
            daemon.start(daemonize=True)
            time.sleep(1)

    except ImportError:
        safe_print("   âŒ Daemon not available, falling back to legacy workers")
        return chaos_test_13_pytorch_lightning_storm()

    for config in test_configs:
        safe_print(f"   ğŸ˜ˆ Testing Storm: {config['name']}")

        config_start = time.perf_counter()
        timings = {}

        try:
            # Create daemon proxy for torch environment
            safe_print("      âš™ï¸  Connecting to daemon worker...")
            boot_start = time.perf_counter()

            proxy = DaemonProxy(client, config["torch"])
            boot_time = time.perf_counter() - boot_start
            timings["worker_connect"] = boot_time

            safe_print(f"      â±ï¸  Worker connected in {boot_time*1000:.2f}ms")

            # Execute code that loads lightning within the torch environment
            code_to_run = f"""
from omnipkg.loader import omnipkgLoader

# We're already in the torch environment, now add lightning
with omnipkgLoader("{config['lightning']}"):
    import pytorch_lightning as pl
    import torch
    import numpy as np
    try:
        from .common_utils import safe_print
    except ImportError:
        from omnipkg.common_utils import safe_print
    # Verify versions
    torch_ver = torch.__version__
    lightning_ver = pl.__version__
    numpy_ver = np.__version__
    
    print(f"âš¡ PyTorch {{torch_ver}} + Lightning {{lightning_ver}} + NumPy {{numpy_ver}} loaded successfully.")
    
    # Quick functionality test
    class SimpleModel(pl.LightningModule):
        def __init__(self):
            super().__init__()
            self.layer = torch.nn.Linear(10, 1)
        
        def forward(self, x):
            return self.layer(x)
    
    model = SimpleModel()
    test_input = torch.randn(5, 10)
    output = model(test_input)
    
    print(f"âœ… Model forward pass: input {{test_input.shape}} -> output {{output.shape}}")
"""

            exec_start = time.perf_counter()
            result = proxy.execute(code_to_run)
            exec_time = time.perf_counter() - exec_start
            timings["execution"] = exec_time

            safe_print(f"      â±ï¸  Execution completed in {exec_time*1000:.2f}ms")

            if result["success"]:
                config_time = time.perf_counter() - config_start
                timings["total"] = config_time
                timing_results.append(
                    {"config": config["name"], "timings": timings, "success": True}
                )

                successful += 1
                if verbose and result.get("stdout"):
                    for line in result["stdout"].strip().split("\n"):
                        safe_print(f"      {line}")
                safe_print(f"      â±ï¸  Total config time: {config_time*1000:.2f}ms")
                safe_print(f"      âœ… LIGHTNING STRIKE #{successful}!")
            else:
                config_time = time.perf_counter() - config_start
                timings["total"] = config_time
                timing_results.append(
                    {
                        "config": config["name"],
                        "timings": timings,
                        "success": False,
                        "error": result.get("error", "Unknown error"),
                    }
                )

                safe_print(f"      â±ï¸  Failed after {config_time*1000:.2f}ms")
                safe_print(f"      ğŸ’¥ Failed: {result.get('error', 'Unknown error')}")
                if verbose and result.get("traceback"):
                    safe_print(f"      Traceback: {result['traceback'][:500]}")

        except Exception as e:
            config_time = time.perf_counter() - config_start
            timings["total"] = config_time
            timing_results.append(
                {
                    "config": config["name"],
                    "timings": timings,
                    "success": False,
                    "error": str(e),
                }
            )

            safe_print(f"      â±ï¸  Exception after {config_time*1000:.2f}ms")
            safe_print(f"      ğŸ’¥ Exception: {str(e)}")

    total_time = time.perf_counter() - total_start

    # Display timing summary
    safe_print("\n   ğŸ“Š TIMING SUMMARY:")
    safe_print(f"   â±ï¸  Total test time: {total_time*1000:.2f}ms")

    if timing_results:
        avg_connect = sum(
            t["timings"].get("worker_connect", 0) for t in timing_results
        ) / len(timing_results)
        avg_exec = sum(
            t["timings"].get("execution", 0)
            for t in timing_results
            if "execution" in t["timings"]
        )
        avg_exec = (
            avg_exec / len([t for t in timing_results if "execution" in t["timings"]])
            if any("execution" in t["timings"] for t in timing_results)
            else 0
        )

        safe_print(f"   â±ï¸  Avg worker connect: {avg_connect*1000:.2f}ms")
        if avg_exec > 0:
            safe_print(f"   â±ï¸  Avg execution: {avg_exec*1000:.2f}ms")

    safe_print(f"\n   ğŸ¯ Compatible Pairs: {successful}/{len(test_configs)} successful")

    if successful == len(test_configs):
        safe_print("   âœ… PYTORCH LIGHTNING STORM SURVIVED!")
        safe_print("\n")
        return True
    else:
        safe_print("   âš¡ LIGHTNING STORM FAILED!")
        safe_print("\n")
        return False


def chaos_test_14_circular_dependency_hell():
    """â­• TEST 14: Create actual circular imports between bubbles"""
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 14: â­• CIRCULAR DEPENDENCY HELL                        â•‘")
    safe_print("â•‘  Package A imports B, B imports A â€” across version bubbles   â•‘")
    safe_print("â•‘  NOW POWERED BY PERSISTENT WORKERS FOR TRUE ISOLATION!       â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    verbose = is_verbose_mode()
    safe_print("ğŸŒ€ Creating circular dependency nightmare...\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Test 1: NumPy â†” Pandas (Nested Loading inside Worker)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("   ğŸ˜ˆ Circle 1: NumPy â†” Pandas Tango (Worker Isolated)")
    worker_1 = PersistentWorker("numpy==1.24.3", verbose=verbose)
    try:
        code = """
from omnipkg.loader import omnipkgLoader
import numpy as np
import sys
try:
    from .common_utils import safe_print
except ImportError:
    from omnipkg.common_utils import safe_print
sys.stderr.write(f"      NumPy 1.24.3 loaded: {np.__version__}\\n")

# Now try to load pandas that depends on different numpy
try:
    with omnipkgLoader("pandas==2.2.0"):
        import pandas as pd
        sys.stderr.write(f"      Pandas 2.2.0 loaded: {pd.__version__}\\n")
        sys.stderr.write(f"      NumPy version inside pandas: {pd.np.__version__ if hasattr(pd, 'np') else 'unknown'}\\n")
        print("SUCCESS")
except Exception as e:
    sys.stderr.write(f"      ğŸ’¥ Pandas failed (expected): {str(e)[:100]}...\\n")
"""
        result = worker_1.execute(code)
        if result["success"]:
            safe_print("      âœ… CIRCULAR DANCE COMPLETED!")
        else:
            safe_print(
                f"      âš ï¸  Circle 1 result: {result.get('error', 'Unknown error')}"
            )
    finally:
        worker_1.shutdown()

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Test 2: Torch â†” NumPy (The C++ Crash Candidate)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("\n   ğŸ˜ˆ Circle 2: Torch â†” NumPy Madness (Worker Isolated)")
    worker_2 = PersistentWorker("torch==2.0.1+cu118", verbose=verbose)
    try:
        code = """
from omnipkg.loader import omnipkgLoader
import torch
import sys

sys.stderr.write(f"      Torch 2.0.1 loaded: {torch.__version__}\\n")

# Torch uses numpy internally, now load different numpy
with omnipkgLoader("numpy==1.24.3"):
    import numpy as np
    sys.stderr.write(f"      NumPy 2.3.5 loaded: {np.__version__}\\n")
    
    # Try to use torch with the new numpy (Cross-boundary interaction)
    result = torch.tensor([1, 2, 3]).numpy()
    sys.stderr.write(f"      Torch â†’ NumPy conversion result: {result}\\n")
"""
        result = worker_2.execute(code)
        if result["success"]:
            safe_print("      âœ… CIRCULAR MADNESS SURVIVED!")
        else:
            safe_print(
                f"      ğŸ’¥ Torch/NumPy circle failed: {result['error'][:100]}..."
            )
    finally:
        worker_2.shutdown()

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Test 4: Rapid Circular Switching (TRUE VERSION SWITCHING)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("\n   ğŸ˜ˆ Circle 4: Rapid Circular Switching (Dual Workers)")
    safe_print("      ğŸ”¥ Booting competing environments...")

    # We keep TWO workers alive and toggle between them
    w_old = PersistentWorker("torch==2.0.1+cu118", verbose=verbose)
    w_new = PersistentWorker("torch==2.1.0", verbose=verbose)

    successes = 0
    try:
        for i in range(10):
            target_worker = w_old if i % 2 == 0 else w_new
            expected = "2.0.1" if i % 2 == 0 else "2.1.0"

            # Simple ping to verify version state
            code = f"""
import torch
import sys
# sys.stderr.write(f"      Round {i+1}: Checking torch version...\\n")
if torch.__version__.startswith("{expected}"):
    print("MATCH")
else:
    raise ValueError(f"Version Mismatch! Got {{torch.__version__}}")
"""
            result = target_worker.execute(code)

            if result["success"]:
                successes += 1
                # Optional: visual feedback
                # sys.stdout.write(f" {expected}")
                # sys.stdout.flush()
            else:
                safe_print(f"      ğŸ’¥ Round {i+1} failed: {result['error']}")
    finally:
        w_old.shutdown()
        w_new.shutdown()

    print(f"\n      Rapid switches: {successes}/10 successful")

    if successes == 10:
        safe_print("      âœ… RAPID CIRCULAR SWITCHING MASTERED! (True Isolation)")
    else:
        safe_print("      âš ï¸  Some circular switches failed")

    safe_print("\nğŸ­ CIRCULAR DEPENDENCY HELL COMPLETE!")
    safe_print("âœ… REAL PACKAGES, REAL CIRCLES, REAL SURVIVAL!\n")


def chaos_test_15_isolation_strategy_benchmark():
    """
    âš¡ TEST 15: COMPREHENSIVE ISOLATION STRATEGY BENCHMARK
    """
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 15: âš¡ ISOLATION STRATEGY BENCHMARK                   â•‘")
    safe_print("â•‘  Compare speed vs isolation trade-offs                      â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    specs = ["torch==2.0.1+cu118", "torch==2.1.0"] * 3  # 6 switches total
    results = {}

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STRATEGY 1: In-Process
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("ğŸ“Š Strategy 1: IN-PROCESS (Baseline)")
    safe_print("   - Pros: Fastest")
    safe_print("   - Cons: Can't actually switch C++ packages")
    safe_print("-" * 60)

    start = time.perf_counter()
    success_count = 0

    for i, spec in enumerate(specs):
        try:
            with omnipkgLoader(spec, quiet=True):  # Quiet to keep benchmark readable
                import torch

                unused = torch.sin(torch.tensor([1.0]))
                success_count += 1
                sys.stdout.write(".")
                sys.stdout.flush()
        except Exception as e:
            safe_print(f"   âŒ Round {i+1} failed: {str(e)[:40]}")

    print()  # Newline
    elapsed_in_process = time.perf_counter() - start
    results["in_process"] = {
        "time": elapsed_in_process,
        "success": success_count,
        "per_switch": elapsed_in_process / len(specs),
    }

    safe_print(
        f"   âœ… Total: {elapsed_in_process:.3f}s ({success_count}/{len(specs)} success)"
    )
    safe_print(f"   âš¡ Per switch: {results['in_process']['per_switch']*1000:.1f}ms\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STRATEGY 2: Standard Subprocess
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("ğŸ“Š Strategy 2: STANDARD SUBPROCESS")
    safe_print("   - Pros: Complete isolation, true switching")
    safe_print("   - Cons: Slow due to full Python startup")
    safe_print("-" * 60)

    start = time.perf_counter()
    success_count = 0

    for i, spec in enumerate(specs):
        # We add a print inside to verify it runs
        code = f"""
from omnipkg.loader import omnipkgLoader
with omnipkgLoader("{spec}", quiet=True):
    import torch
    _ = torch.sin(torch.tensor([1.0]))
    print(f"   [Subprocess] {spec} active")
"""
        if run_python_code_in_isolation(code, f"Subprocess {i+1}"):
            success_count += 1

    elapsed_subprocess = time.perf_counter() - start
    results["subprocess"] = {
        "time": elapsed_subprocess,
        "success": success_count,
        "per_switch": elapsed_subprocess / len(specs),
    }

    safe_print(
        f"   âœ… Total: {elapsed_subprocess:.3f}s ({success_count}/{len(specs)} success)"
    )
    safe_print(f"   âš¡ Per switch: {results['subprocess']['per_switch']*1000:.1f}ms\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STRATEGY 3: Optimized Subprocess
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("ğŸ“Š Strategy 3: OPTIMIZED SUBPROCESS")
    safe_print("   - Pros: Faster startup with minimal imports")
    safe_print("   - Cons: Still spawning full processes")
    safe_print("-" * 60)

    start = time.perf_counter()
    success_count = 0

    for i, spec in enumerate(specs):
        code = f"""
import sys
try:
    import omnipkg
except ImportError:
    # Fallback if installed in editable mode
    sys.path.insert(0, "{Path(__file__).parent.parent.parent}")

from omnipkg.loader import omnipkgLoader
with omnipkgLoader("{spec}", quiet=True):
    import torch
    torch.sin(torch.tensor([1.0]))
    print(f"   [Optimized] {spec} calculated")
"""
        result = subprocess.run(
            [sys.executable, "-c", code], capture_output=True, text=True, timeout=10
        )
        if result.returncode == 0:
            success_count += 1
            # Print the output from the subprocess so we see it live
            print(result.stdout.strip())

    elapsed_optimized = time.perf_counter() - start
    results["optimized_subprocess"] = {
        "time": elapsed_optimized,
        "success": success_count,
        "per_switch": elapsed_optimized / len(specs),
    }

    safe_print(
        f"   âœ… Total: {elapsed_optimized:.3f}s ({success_count}/{len(specs)} success)"
    )
    safe_print(
        f"   âš¡ Per switch: {results['optimized_subprocess']['per_switch']*1000:.1f}ms\n"
    )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STRATEGY 4: Fork-based (Unix only)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if hasattr(os, "fork"):
        safe_print("ğŸ“Š Strategy 4: FORK-BASED ISOLATION (Unix)")
        start = time.perf_counter()
        success_count = 0
        for i, spec in enumerate(specs):
            pid = os.fork()
            if pid == 0:
                try:
                    with omnipkgLoader(spec, quiet=True):
                        import torch

                        unused = torch.sin(torch.tensor([1.0]))
                        print(f"   [Fork] {spec} done")
                    sys.exit(0)
                except Exception:
                    sys.exit(1)
            else:
                _, status = os.waitpid(pid, 0)
                if os.WIFEXITED(status) and os.WEXITSTATUS(status) == 0:
                    success_count += 1
        elapsed_fork = time.perf_counter() - start
        results["fork"] = {
            "time": elapsed_fork,
            "success": success_count,
            "per_switch": elapsed_fork / len(specs),
        }
        safe_print(
            f"   âœ… Total: {elapsed_fork:.3f}s ({success_count}/{len(specs)} success)"
        )
        safe_print(f"   âš¡ Per switch: {results['fork']['per_switch']*1000:.1f}ms\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STRATEGY 5: Persistent Worker Pool (THE MAIN EVENT)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("ğŸ“Š Strategy 5: PERSISTENT WORKER POOL")
    safe_print("   - Pros: Reuse processes, amortize startup cost")
    safe_print("   - Visibility: ğŸŸ¢ LIVE LOGGING ENABLED")
    safe_print("-" * 60)

    start = time.perf_counter()
    success_count = 0
    workers = {}

    try:
        # 1. Initialize Workers (Verbose = True shows the boot logs)
        safe_print("   âš™ï¸  Booting workers (One-time cost)...")
        for spec in set(specs):
            # PASS verbose=True HERE!
            workers[spec] = PersistentWorker(spec, verbose=True)

        safe_print("\n   ğŸš€ Starting High-Speed Switching Loop...")

        # 2. Run the Loop
        for i, spec in enumerate(specs):
            try:
                # We inject a print into the worker so you see it responding live!
                # Since PersistentWorker streams stderr to your console, this will show up.
                code = "import torch; import sys; sys.stderr.write(f'   âš¡ [Worker {torch.__version__}] Calculation complete\\n')"

                result = workers[spec].execute(code)

                if result["success"]:
                    success_count += 1
            except Exception as e:
                safe_print(f"   âŒ Round {i+1} failed: {str(e)[:40]}")
    finally:
        safe_print("   ğŸ›‘ Shutting down worker pool...")
        for worker in workers.values():
            worker.shutdown()

    elapsed_worker = time.perf_counter() - start
    results["worker_pool"] = {
        "time": elapsed_worker,
        "success": success_count,
        "per_switch": elapsed_worker / len(specs),
    }

    safe_print(
        f"   âœ… Total: {elapsed_worker:.3f}s ({success_count}/{len(specs)} success)"
    )
    safe_print(f"   âš¡ Per switch: {results['worker_pool']['per_switch']*1000:.1f}ms\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STRATEGY 3: Daemon JSON (Control Plane)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("ğŸ“Š Strategy 3: DAEMON (JSON Mode)")
    safe_print("   - Pros: Persistent workers, no boot cost")
    safe_print("   - Cons: JSON serialization overhead")
    safe_print("-" * 60)

    try:
        from omnipkg.isolation.worker_daemon import (
            DaemonClient,
            DaemonProxy,
            WorkerPoolDaemon,
        )

        # Ensure daemon is up
        client = DaemonClient()
        if not client.status().get("success"):
            safe_print("   âš™ï¸  Starting Daemon...")
            WorkerPoolDaemon().start(daemonize=True)
            time.sleep(2)

        start = time.perf_counter()
        success_count = 0

        for spec in specs:
            proxy = DaemonProxy(client, spec)
            res = proxy.execute("import torch; print('ok')")
            if res["success"]:
                success_count += 1

        elapsed = time.perf_counter() - start
        results["daemon_json"] = {"time": elapsed, "success": success_count}
        safe_print(f"   âœ… Total: {elapsed:.3f}s")
        safe_print(f"   âš¡ Per switch: {elapsed/6*1000:.1f}ms\n")

    except ImportError:
        pass

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STRATEGY 4: Daemon Zero-Copy SHM (Data Plane)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("ğŸ“Š Strategy 4: DAEMON (Zero-Copy SHM)")
    safe_print("   - Pros: Persistent workers, zero-copy data")
    safe_print("   - Cons: Tiny SHM setup overhead for small data")
    safe_print("-" * 60)

    try:
        import numpy as np

        data = np.array([1.0])  # Tiny payload

        start = time.perf_counter()
        success_count = 0

        for spec in specs:
            try:
                res_arr, unused = client.execute_zero_copy(
                    spec, "arr_out[0] = 1", data, (1,), "float64"
                )
                success_count += 1
            except:
                pass

        elapsed = time.perf_counter() - start
        results["daemon_shm"] = {"time": elapsed, "success": success_count}
        safe_print(f"   âœ… Total: {elapsed:.3f}s")
        safe_print(f"   âš¡ Per switch: {elapsed/6*1000:.1f}ms\n")

    except ImportError:
        pass

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # FINAL SCOREBOARD
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("\n" + "=" * 60)
    safe_print(f"{'STRATEGY':<25} | {'TOTAL':<8} | {'PER SWAP':<10} | {'VS BASELINE'}")
    safe_print("-" * 60)

    baseline = results["in_process"]["time"]

    sorted_results = sorted(results.items(), key=lambda x: x[1]["time"])

    for strat, data in sorted_results:
        t = data["time"]
        per = (t / 6) * 1000

        if t < baseline:
            comp = f"{baseline/t:.1f}x FASTER"
        else:
            comp = f"{t/baseline:.1f}x SLOWER"

        safe_print(f"{strat:<25} | {t:6.3f}s | {per:6.1f}ms | {comp}")

    safe_print("=" * 60 + "\n")


def chaos_test_16_nested_reality_hell():
    """ğŸ§¬ TEST 16: NESTED REALITY HELL"""
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 16: ğŸ§¬  NESTED REALITY HELL                            â•‘")
    safe_print("â•‘  Phase 1: Multi-Process Switching | Phase 2: Deep Nesting    â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    verbose = is_verbose_mode()

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 1: Rapid Sequential NumPy Switching (Using omnipkgLoader)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("   ğŸ“ PHASE 1: Rapid Sequential NumPy Switching (Context Manager)")
    safe_print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    versions = [
        ("1.24.3", "numpy==1.24.3"),
        ("1.26.4", "numpy==1.26.4"),
        ("2.3.5", "numpy==2.3.5"),
    ]

    phase1_success = True

    for expected_ver, spec in versions:
        try:
            with omnipkgLoader(spec, quiet=not verbose):
                import numpy as np

                actual_ver = np.__version__
                arr = np.array([1, 2, 3, 4, 5])
                mean = arr.mean()

                if actual_ver == expected_ver:
                    safe_print(
                        f"     âœ… {spec:<15} â†’ Active (version={actual_ver}, mean={mean})"
                    )
                else:
                    safe_print(
                        f"     âŒ {spec:<15} â†’ Mismatch! Expected {expected_ver}, got {actual_ver}"
                    )
                    phase1_success = False

        except Exception as e:
            safe_print(f"     ğŸ’¥ {spec:<15} â†’ Failed: {e}")
            phase1_success = False

        time.sleep(0.1)  # Brief pause between switches

    safe_print(f"   ğŸ¯ Phase 1 Result: {'PASSED' if phase1_success else 'FAILED'}\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 2: 7-Layer Deep Nested Activation
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("   ğŸ“ PHASE 2: 7-Layer Deep Nested Activation (Overlay)")
    safe_print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    safe_print("   âš™ï¸  Booting base worker (TensorFlow)...")

    tf_worker = PersistentWorker("tensorflow==2.13.0", verbose=verbose)

    try:
        nested_hell_code = """
from omnipkg.loader import omnipkgLoader
import sys
import site
import os
try:
    from .common_utils import safe_print
except ImportError:
    from omnipkg.common_utils import safe_print
def log(msg):
    sys.stderr.write(msg + '\\n')
    sys.stderr.flush()

# Restore main env visibility
import omnipkg
main_site_packages = os.path.dirname(os.path.dirname(omnipkg.__file__))
if main_site_packages not in sys.path:
    sys.path.append(main_site_packages)

log("     ğŸ”„ Starting nested overlay stack...")

# Layer 1: NumPy 1.24.3
with omnipkgLoader("numpy==1.24.3", quiet=False, isolation_mode='overlay'):
    import numpy as np
    
    # Layer 2: SciPy 1.10.1 (Compatible with NumPy 1.24)
    with omnipkgLoader("scipy==1.10.1", quiet=False, isolation_mode='overlay'):
        import scipy
        import scipy.linalg
        
        # Layer 3: Pandas 2.0.3 (Compatible with NumPy 1.24)
        with omnipkgLoader("pandas==2.0.3", quiet=False, isolation_mode='overlay'):
            import pandas as pd
            
            # Layer 4: Scikit-Learn 1.3.2
            with omnipkgLoader("scikit-learn==1.3.2", quiet=False, isolation_mode='overlay'):
                from sklearn.ensemble import RandomForestClassifier
                
                # TF from base
                import tensorflow as tf
                
                # Layer 5: PyTorch 2.0.1
                with omnipkgLoader("torch==2.0.1+cu118", quiet=False, isolation_mode='overlay'):
                    import torch
                    
                    log("     âœ… ALL LAYERS LOADED!")
                    
                    tf_tens = tf.constant([1,2,3])
                    torch_tens = torch.tensor([1,2,3])
                    sp_val = scipy.linalg.norm([1,2,3])
                    
                    log(f"     ğŸ‰ Verification: TF={tf_tens.shape}, Torch={torch_tens.shape}, SciPy={sp_val:.2f}")

print("SUCCESS")
"""
        result = tf_worker.execute(nested_hell_code)

        if result["success"] and "SUCCESS" in result["stdout"]:
            safe_print("\n   âœ… Phase 2: 7-layer stack STABLE!")
            phase2_success = True
        else:
            safe_print(
                f"\n   ğŸ’¥ Phase 2 COLLAPSED: {result.get('error', result.get('stderr'))}\n"
            )
            phase2_success = False

    finally:
        tf_worker.shutdown()

    if phase1_success and phase2_success:
        safe_print("\nâœ… NESTED REALITY CONQUERED! (Multi-process + Overlay)")
        return True
    else:
        return False


def chaos_test_17_triple_python_multiverse():
    """ğŸŒŒ TEST 17: TRIPLE PYTHON MULTIVERSE - THE ULTIMATE DEMO

    This test does something LITERALLY IMPOSSIBLE anywhere else:
    - 3 different Python interpreters (3.9, 3.10, 3.11)
    - Each running different TensorFlow + PyTorch versions
    - All executing CONCURRENTLY in the same process
    - Zero-copy data transfer via SHM between them
    - No Docker, no VMs, no serialization overhead

    This replaces Test 17 (experimental) with something that actually works!
    """
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 17: ğŸŒŒ TRIPLE PYTHON MULTIVERSE                       â•‘")
    safe_print("â•‘  3 Pythons Ã— 2 Frameworks Ã— Concurrent Execution = IMPOSSIBLEâ•‘")
    safe_print("â•‘  ...except with omnipkg daemon + zero-copy SHM! ğŸš€           â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    # Check if we have the omnipkg CLI available for Python management
    try:
        result = subprocess.run(
            ["omnipkg", "info", "python"], capture_output=True, text=True, timeout=5
        )
        if result.returncode != 0:
            safe_print("   âš ï¸  omnipkg Python management not available")
            safe_print("   ğŸ’¡ This test requires: omnipkg python adopt 3.9/3.10/3.11")
            safe_print("   â­ï¸  SKIPPING (optional feature)\n")
            return True
    except (subprocess.TimeoutExpired, FileNotFoundError):
        safe_print("   âš ï¸  omnipkg CLI not available")
        safe_print("   â­ï¸  SKIPPING (optional feature)\n")
        return True

    # Initialize daemon
    try:
        from omnipkg.isolation.worker_daemon import DaemonClient, WorkerPoolDaemon
        import numpy as np
        import threading
        import subprocess as sp  # Use alias to avoid conflicts

        client = DaemonClient()
        if not client.status().get("success"):
            safe_print("   âš™ï¸  Starting Multiverse Daemon...")
            # Use subprocess via CLI
            result = sp.run(
                [sys.executable, "-m", "omnipkg.isolation.worker_daemon", "start"],
                capture_output=True,
                text=True,
            )
            time.sleep(2)

            # Verify it started
            client = DaemonClient()
            status = client.status()
            if not status.get("success"):
                safe_print("   âŒ Daemon failed to start")
                if result.stderr:
                    safe_print(f"   Error: {result.stderr[:200]}")
                return False
            safe_print("   âœ… Daemon started successfully")
    except ImportError:
        safe_print("   âŒ Daemon not available")
        return False

    # Define our parallel universes
    universes = [
        {
            "name": "Universe Alpha",
            "python": "3.9",
            "tf_spec": "tensorflow==2.12.0",
            "torch_spec": "torch==2.0.1+cu118",
            "emoji": "ğŸ”´",
            "operation": "Generate random matrix (1000x1000)",
        },
        {
            "name": "Universe Beta",
            "python": "3.10",
            "tf_spec": "tensorflow==2.13.0",
            "torch_spec": "torch==2.1.0",
            "emoji": "ğŸŸ¢",
            "operation": "Compute matrix determinant",
        },
        {
            "name": "Universe Gamma",
            "python": "3.11",
            "tf_spec": "tensorflow==2.20.0",
            "torch_spec": "torch==2.2.0+cu121",
            "emoji": "ğŸ”µ",
            "operation": "Apply neural network layer",
        },
    ]

    for u in universes:
        safe_print(f"   {u['emoji']} {u['name']:<20} Python {u['python']}")
        safe_print(f"      â”œâ”€ TensorFlow: {u['tf_spec']}")
        safe_print(f"      â”œâ”€ PyTorch:    {u['torch_spec']}")
        safe_print(f"      â””â”€ Task:       {u['operation']}")

    safe_print("\n   " + "â”€" * 60)
    safe_print("   ğŸ¯ MISSION: Pass data through all 3 universes via SHM\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 1: Verify Python Interpreters Available
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("   ğŸ“ PHASE 1: Checking Python Interpreters")
    safe_print("   " + "â”€" * 60)

    available_pythons = {}

    def check_python_version(version):
        """Check if a Python version is available."""
        try:
            result = subprocess.run(
                ["omnipkg", "info", "python"], capture_output=True, text=True, timeout=5
            )
            for line in result.stdout.splitlines():
                if f"Python {version}:" in line:
                    # Extract path
                    parts = line.split(":", 1)
                    if len(parts) == 2:
                        path = parts[1].strip().split()[0]
                        return path
        except:
            pass
        return None

    all_available = True
    for u in universes:
        py_path = check_python_version(u["python"])
        if py_path:
            available_pythons[u["python"]] = py_path
            safe_print(f"   âœ… Python {u['python']}: {py_path}")
        else:
            safe_print(f"   âŒ Python {u['python']}: NOT AVAILABLE")
            safe_print(f"      ğŸ’¡ Install with: omnipkg python adopt {u['python']}")
            all_available = False

    if not all_available:
        safe_print("\n   âš ï¸  Not all Python versions available")
        safe_print("   â­ï¸  SKIPPING (requires python version management)\n")
        return True

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 2: Sequential Baseline (Traditional Approach)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("\n   ğŸ“ PHASE 2: Sequential Baseline (Traditional)")
    safe_print("   " + "â”€" * 60)
    safe_print("   ğŸ“Š Running each universe one-by-one...\n")

    sequential_times = []
    sequential_start = time.perf_counter()

    # Generate initial data
    np.random.rand(100, 100)  # Smaller for faster demo

    for i, u in enumerate(universes):
        safe_print(f"   {u['emoji']} {u['name']} starting...")
        iter_start = time.perf_counter()

        # Execute in the appropriate Python + framework
        code = f"""
import sys
import os
try:
    from .common_utils import safe_print
except ImportError:
    from omnipkg.common_utils import safe_print
# DIAGNOSTIC: Print what Python we're actually using
sys.stderr.write(f"\\n   ğŸ” DIAGNOSTIC: Running in Python {{sys.version}}\\n")
sys.stderr.write(f"   ğŸ” Executable: {{sys.executable}}\\n")
sys.stderr.write(f"   ğŸ” sys.path[0]: {{sys.path[0]}}\\n")

from omnipkg.loader import omnipkgLoader

# Load the framework
with omnipkgLoader("{u['torch_spec']}", quiet=False):  # quiet=False for visibility
    import torch
    
    # DIAGNOSTIC: Verify which torch we loaded
    sys.stderr.write(f"   ğŸ” PyTorch version: {{torch.__version__}}\\n")
    sys.stderr.write(f"   ğŸ” PyTorch path: {{torch.__file__}}\\n")
    
    # Simulate some work
    x = torch.randn(100, 100)
    y = torch.matmul(x, x.T)
    result = torch.trace(y).item()
    
    sys.stderr.write(f"      âœ“ PyTorch {{torch.__version__}} computed trace: {{result:.2f}}\\n")
"""

        # Execute via daemon with specific Python version
        try:
            # Get the Python executable path for this universe
            target_python_exe = available_pythons[u["python"]]

            # CRITICAL: Use client.execute_shm() with python_exe parameter
            # This creates an isolated worker process without swapping main environment
            result = client.execute_shm(
                spec=u["torch_spec"],
                code=code,
                shm_in={},
                shm_out={},
                python_exe=target_python_exe,
            )

            iter_time = (time.perf_counter() - iter_start) * 1000
            sequential_times.append(iter_time)

            if result["success"]:
                safe_print(
                    f"   {u['emoji']} {u['name']} completed in {iter_time:.2f}ms"
                )
            else:
                error_msg = result.get("error", "Unknown")
                safe_print(f"   {u['emoji']} {u['name']} FAILED: {error_msg}")
                # Print full traceback if available
                if result.get("traceback"):
                    safe_print(f"      Traceback: {result['traceback']}")

        except Exception as e:
            safe_print(f"   {u['emoji']} {u['name']} ERROR: {str(e)}")
            # Print full traceback
            import traceback

            safe_print(f"      {traceback.format_exc()}")
            sequential_times.append(float("inf"))

    total_sequential = time.perf_counter() - sequential_start
    safe_print(f"\n   â±ï¸  Sequential Total: {total_sequential*1000:.2f}ms")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 3: Concurrent Execution (THE IMPOSSIBLE DEMO)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("\n   ğŸ“ PHASE 3: CONCURRENT MULTIVERSE (The Impossible)")
    safe_print("   " + "â”€" * 60)
    safe_print("   ğŸš€ All 3 universes executing SIMULTANEOUSLY...\n")

    concurrent_results = {}
    concurrent_lock = threading.Lock()

    def execute_universe(universe):
        """Execute computation in one universe."""
        u_name = universe["name"]
        emoji = universe["emoji"]

        try:
            # Get the actual Python path
            python_exe = available_pythons[universe["python"]]

            start = time.perf_counter()

            code = f"""
import sys
try:
    from .common_utils import safe_print
except ImportError:
    from omnipkg.common_utils import safe_print
sys.stderr.write(f"\\n   ğŸ” [{u_name}] Python: {{sys.version}}\\n")
sys.stderr.write(f"   ğŸ” [{u_name}] Executable: {{sys.executable}}\\n")

from omnipkg.loader import omnipkgLoader
with omnipkgLoader("{universe['torch_spec']}", quiet=False):
    import torch
    sys.stderr.write(f"   ğŸ” [{u_name}] PyTorch: {{torch.__version__}} from {{torch.__file__}}\\n")
    x = torch.randn(200, 200)
    y = torch.matmul(x, x.T)
    result = torch.trace(y).item()
    sys.stderr.write(f"{{result:.6f}}\\n")
"""

            # CRITICAL: Use client.execute_shm() with python_exe, not DaemonProxy
            # This creates isolated worker without swapping main environment
            result = client.execute_shm(
                spec=universe["torch_spec"],
                code=code,
                shm_in={},
                shm_out={},
                python_exe=python_exe,
            )

            elapsed = (time.perf_counter() - start) * 1000

            with concurrent_lock:
                concurrent_results[u_name] = {
                    "success": result["success"],
                    "time": elapsed,
                    "emoji": emoji,
                    "error": result.get("error") if not result["success"] else None,
                    "stderr": result.get("stderr", ""),
                }

                if result["success"]:
                    safe_print(f"   {emoji} {u_name:<20} âœ… {elapsed:>7.2f}ms")
                    # Print diagnostic output
                    if result.get("stderr"):
                        safe_print(f"\n   ğŸ“‹ [{u_name}] Diagnostics:")
                        for line in result["stderr"].strip().split("\n"):
                            if line.strip():
                                safe_print(f"      {line}")
                else:
                    error_msg = result.get("error", "Unknown")
                    safe_print(f"   {emoji} {u_name:<20} âŒ FAILED")
                    safe_print(f"      Error: {error_msg}")
                    if result.get("traceback"):
                        safe_print(f"      Traceback: {result['traceback'][:500]}")
                    if result.get("stderr"):
                        safe_print(f"      Stderr: {result['stderr'][:500]}")

        except Exception as e:
            with concurrent_lock:
                concurrent_results[u_name] = {
                    "success": False,
                    "time": 0,
                    "emoji": emoji,
                    "error": str(e),
                }
                safe_print(f"   {emoji} {u_name:<20} âŒ {str(e)}")
                # Print traceback
                import traceback

                safe_print(f"      {traceback.format_exc()[:500]}")

    # Launch all universes concurrently
    concurrent_start = time.perf_counter()
    threads = []

    for u in universes:
        t = threading.Thread(target=execute_universe, args=(u,))
        threads.append(t)
        t.start()

    # Wait for all to complete
    for t in threads:
        t.join()

    total_concurrent = time.perf_counter() - concurrent_start

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 4: Zero-Copy Data Pipeline Demo
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("\n   ğŸ“ PHASE 4: Zero-Copy Data Pipeline")
    safe_print("   " + "â”€" * 60)
    safe_print("   ğŸ’¾ Passing 1MB matrix through 3 frameworks via SHM\n")

    # Create 1MB test data
    pipeline_data = np.random.rand(500, 250)  # ~1MB
    safe_print(
        f"   ğŸ“¦ Input: {pipeline_data.shape} array ({pipeline_data.nbytes/1024/1024:.2f} MB)"
    )

    pipeline_times = []

    # Stage 1: NumPy â†’ PyTorch (Universe Alpha)
    safe_print("\n   ğŸ”´ Stage 1: Processing in Universe Alpha (PyTorch 2.0)...")
    stage1_start = time.perf_counter()

    stage1_code = """
import torch
import sys
try:
    from .common_utils import safe_print
except ImportError:
    from omnipkg.common_utils import safe_print
sys.stderr.write(f"   ğŸ” [Stage 1] PyTorch: {torch.__version__} from {torch.__file__}\\n")

# arr_in comes from SHM
# Use copy() to fix ABI incompatibility between numpy/torch versions across processes
torch_tensor = torch.from_numpy(arr_in.copy())
result = torch.nn.functional.relu(torch_tensor)
arr_out[:] = result.numpy()
"""

    try:
        stage1_out, unused = client.execute_zero_copy(
            universes[0]["torch_spec"],
            stage1_code,
            input_array=pipeline_data,
            output_shape=pipeline_data.shape,
            output_dtype="float64",
            python_exe=available_pythons[universes[0]["python"]],
        )
        stage1_time = (time.perf_counter() - stage1_start) * 1000
        pipeline_times.append(stage1_time)

        # Print diagnostic output if available
        if "stderr" in unused and unused["stderr"]:
            safe_print(f"      Diagnostic: {unused['stderr'][:200]}")

        safe_print(f"   âœ… Stage 1 complete: {stage1_time:.2f}ms (zero-copy SHM)")

        # Stage 2: Universe Beta
        safe_print("\n   ğŸŸ¢ Stage 2: Processing in Universe Beta (PyTorch 2.1)...")
        stage2_start = time.perf_counter()

        stage2_code = """
import torch
import sys
try:
    from .common_utils import safe_print
except ImportError:
    from omnipkg.common_utils import safe_print
sys.stderr.write(f"   ğŸ” [Stage 2] PyTorch: {torch.__version__} from {torch.__file__}\\n")

torch_tensor = torch.from_numpy(arr_in.copy())
result = torch.sigmoid(torch_tensor)
arr_out[:] = result.numpy()
"""

        stage2_out, unused = client.execute_zero_copy(
            universes[1]["torch_spec"],
            stage2_code,
            input_array=stage1_out,
            output_shape=stage1_out.shape,
            output_dtype="float64",
            python_exe=available_pythons[universes[1]["python"]],
        )
        stage2_time = (time.perf_counter() - stage2_start) * 1000
        pipeline_times.append(stage2_time)
        safe_print(f"   âœ… Stage 2 complete: {stage2_time:.2f}ms (zero-copy SHM)")

        # Stage 3: Universe Gamma
        safe_print("\n   ğŸ”µ Stage 3: Processing in Universe Gamma (PyTorch 2.2)...")
        stage3_start = time.perf_counter()

        stage3_code = """
import torch
import sys
try:
    from .common_utils import safe_print
except ImportError:
    from omnipkg.common_utils import safe_print
sys.stderr.write(f"   ğŸ” [Stage 3] PyTorch: {torch.__version__} from {torch.__file__}\\n")

torch_tensor = torch.from_numpy(arr_in.copy())
result = torch.tanh(torch_tensor)
arr_out[:] = result.numpy()
"""

        stage3_out, unused = client.execute_zero_copy(
            universes[2]["torch_spec"],
            stage3_code,
            input_array=stage2_out,
            output_shape=stage2_out.shape,
            output_dtype="float64",
            python_exe=available_pythons[universes[2]["python"]],
        )
        stage3_time = (time.perf_counter() - stage3_start) * 1000
        pipeline_times.append(stage3_time)
        safe_print(f"   âœ… Stage 3 complete: {stage3_time:.2f}ms (zero-copy SHM)")

        total_pipeline = sum(pipeline_times)
        safe_print(f"\n   ğŸ¯ Pipeline Total: {total_pipeline:.2f}ms")
        safe_print("   ğŸ’¡ Data passed through 3 frameworks with ZERO serialization!")

    except Exception as e:
        safe_print(f"   âŒ Pipeline failed: {str(e)}")
        import traceback

        safe_print(f"      {traceback.format_exc()[:1000]}")
        total_pipeline = float("inf")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # FINAL RESULTS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("\n" + "â•" * 66)
    safe_print("   ğŸ“Š FINAL RESULTS: THE IMPOSSIBLE IS POSSIBLE")
    safe_print("â•" * 66)

    safe_print(f"\n   {'METRIC':<30} | {'TIME':<15}")
    safe_print("   " + "â”€" * 50)
    safe_print(f"   {'Sequential (traditional)':<30} | {total_sequential*1000:>7.2f}ms")
    safe_print(f"   {'Concurrent (3 parallel)':<30} | {total_concurrent*1000:>7.2f}ms")

    if total_concurrent < total_sequential:
        speedup = total_sequential / total_concurrent
        safe_print(f"   {'SPEEDUP':<30} | {speedup:>7.2f}x")

    if pipeline_times:
        safe_print(
            f"   {'Zero-copy pipeline (3 stages)':<30} | {total_pipeline:>7.2f}ms"
        )

    safe_print("\n   ğŸ’¡ WHAT MAKES THIS IMPOSSIBLE ELSEWHERE:")
    safe_print("   " + "â”€" * 60)
    safe_print("   âŒ Docker containers: Need serialization between containers")
    safe_print("   âŒ Virtual environments: Can't run multiple Pythons concurrently")
    safe_print("   âŒ Traditional isolation: No zero-copy data transfer")
    safe_print("   âœ… omnipkg daemon + SHM: All of the above, ZERO overhead!")

    safe_print("\n   ğŸ† VERDICT:")
    successful_concurrent = sum(1 for r in concurrent_results.values() if r["success"])

    if successful_concurrent == len(universes) and pipeline_times:
        safe_print("   âœ… TRIPLE PYTHON MULTIVERSE: COMPLETE SUCCESS!")
        safe_print("   ğŸŒŒ Concurrent execution: STABLE")
        safe_print("   ğŸ’¾ Zero-copy pipeline: OPERATIONAL")
        safe_print("   ğŸš€ Performance: BLAZING FAST")
        safe_print("\n   ğŸ‰ WE JUST DID THE IMPOSSIBLE! ğŸ‰\n")
        return True
    elif successful_concurrent >= 2:
        safe_print("   âœ… MULTIVERSE OPERATIONAL (partial)")
        return True
    else:
        safe_print("   âš ï¸  Some universes collapsed")
        return False


def chaos_test_18_worker_pool_drag_race():
    """ğŸï¸ TEST 18: HFT SIMULATION - High Frequency Worker Swapping"""
    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 18: ğŸï¸  HFT SIMULATION (Worker Pool Drag Race)        â•‘")
    safe_print("â•‘  Scenario: 4 Concurrent Threads hammering the Daemon         â•‘")
    safe_print("â•‘  Goal: Prove thread-safety and max throughput                â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    # 1. Setup the "Trading Floor" (Daemon)
    try:
        from omnipkg.isolation.worker_daemon import (
            DaemonClient,
            DaemonProxy,
            WorkerPoolDaemon,
        )

        client = DaemonClient()
        if not client.status().get("success"):
            safe_print("   âš™ï¸  Starting Trading Floor (Daemon)...")
            # VIP list ensures our workers are warm
            vip_specs = [
                "torch==2.0.1+cu118",
                "torch==2.1.0",
                "numpy==1.24.3",
                "numpy==1.26.4",
            ]
            WorkerPoolDaemon(warmup_specs=vip_specs).start(daemonize=True)
            time.sleep(2)  # Give it a moment to boot the fleet
    except ImportError:
        return False

    # 2. Define the Workload
    # Two threads want Torch 2.0, Two threads want Torch 2.1
    # They will fight for the workers.

    LAPS = 50  # 50 requests per thread
    THREADS = 4

    safe_print(
        f"   ğŸš¦ RACE SETTINGS: {THREADS} Threads x {LAPS} Laps = {THREADS*LAPS} Total Transactions"
    )
    safe_print("   ğŸï¸  Drivers to your engines...")

    start_gun = threading.Event()
    results = []

    def hft_trader(thread_id, spec):
        # Create a proxy for this thread
        proxy = DaemonProxy(client, spec)

        # Wait for gun
        start_gun.wait()

        t_start = time.perf_counter()
        success_count = 0

        # The payload: extremely fast execution
        code = "x = 1 + 1"

        for unused in range(LAPS):
            res = proxy.execute(code)
            if res["success"]:
                success_count += 1

        t_end = time.perf_counter()
        results.append(
            {
                "id": thread_id,
                "spec": spec,
                "time": t_end - t_start,
                "success": success_count,
            }
        )

    # 3. Create Threads
    threads = []
    specs = ["torch==2.0.1+cu118", "torch==2.1.0", "numpy==1.24.3", "numpy==1.26.4"]

    for i in range(THREADS):
        t = threading.Thread(target=hft_trader, args=(i, specs[i % len(specs)]))
        threads.append(t)
        t.start()

    # 4. START RACE
    safe_print("   ğŸ”« GO!")
    time.sleep(0.5)  # Let threads initialize
    race_start = time.perf_counter()
    start_gun.set()

    for t in threads:
        t.join()

    total_race_time = time.perf_counter() - race_start

    # 5. Analysis
    total_reqs = sum(r["success"] for r in results)
    safe_print("\n   ğŸ FINISH LINE")
    safe_print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    for r in results:
        tps = r["success"] / r["time"]
        safe_print(
            f"   ğŸï¸  Thread {r['id']} ({r['spec']}): {r['success']}/{LAPS} ok | {r['time']*1000/LAPS:.2f}ms/req | {tps:.1f} req/s"
        )

    safe_print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    safe_print(f"   â±ï¸  Total Wall Time: {total_race_time:.3f}s")
    safe_print(
        f"   âš¡ System Throughput: {total_reqs / total_race_time:.1f} Transactions/Second"
    )

    if total_reqs == THREADS * LAPS:
        safe_print("\n   ğŸ† RESULT: MARKET STABLE. ZERO DROPPED PACKETS.")
    else:
        safe_print("\n   âš ï¸  RESULT: PACKET LOSS DETECTED.")

    return True


def chaos_test_19_zero_copy_hft():
    """ğŸš€ TEST 19: ZERO-COPY vs JSON (10MB BENCHMARK)"""
    import numpy as np
    import time
    from omnipkg.isolation.worker_daemon import DaemonClient

    safe_print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 19: ğŸš€ ZERO-COPY vs JSON (10MB BENCHMARK)             â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    client = DaemonClient()
    if not client.status().get("success"):
        safe_print("   âŒ Daemon not running")
        return

    # 1. Create 10MB Matrix
    size = (1000, 1250)
    safe_print(f"   ğŸ“‰ Generating 10MB Matrix {size}...")
    data = np.random.rand(*size)
    safe_print("   âœ… Data ready.\n")

    spec = "numpy==1.26.4"

    # ---------------------------------------------------------
    # ROUND 1: JSON
    # ---------------------------------------------------------
    safe_print("   ğŸ¢ ROUND 1: Standard JSON Serialization")
    print("      (This will take ~7 seconds...)")
    start = time.perf_counter()

    try:
        input_list = data.tolist()
        json_code = f"import numpy as np; arr = np.array({input_list}); result={{'out': (arr*2).tolist()}}"
        res = client.execute_shm(spec, json_code, {}, {})

        if res.get("success"):
            unused = np.array(res["out"])
            duration_json = (time.perf_counter() - start) * 1000
            safe_print(f"      â±ï¸  Total: {duration_json:.2f}ms")
        else:
            safe_print(f"      ğŸ’¥ JSON Failed: {res.get('error')}")
            duration_json = float("inf")
    except Exception as e:
        safe_print(f"      ğŸ’¥ JSON Exception: {e}")
        duration_json = float("inf")

    # ---------------------------------------------------------
    # ROUND 2: SHM
    # ---------------------------------------------------------
    safe_print("\n   ğŸš€ ROUND 2: Shared Memory Pointer Handoff")
    shm_code = "arr_out[:] = arr_in * 2"

    start = time.perf_counter()
    try:
        # UPDATED LINE: Unpack the tuple
        result, unused = client.execute_zero_copy(
            spec, shm_code, input_array=data, output_shape=size, output_dtype="float64"
        )
        duration_shm = (time.perf_counter() - start) * 1000
        safe_print(f"      â±ï¸  Total: {duration_shm:.2f}ms")

        # FINAL SCORE
        safe_print("\n   ğŸ RACE RESULTS (10MB Payload)")
        safe_print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        safe_print(f"   ğŸ¢ JSON: {duration_json:7.2f}ms")
        safe_print(f"   ğŸš€ SHM:  {duration_shm:7.2f}ms")

        if duration_shm > 0:
            speedup = duration_json / duration_shm
            safe_print(f"   ğŸ† Speedup: {speedup:.1f}x FASTER")

    except Exception as e:
        safe_print(f"      ğŸ’¥ SHM Failed: {e}")
        import traceback

        traceback.print_exc()


def chaos_test_20_gpu_resident_pipeline():
    """ğŸ”¥ GPU-RESIDENT ZERO-COPY: Data never leaves VRAM!"""

    # Check if we can import torch (try with omnipkgLoader if needed)
    try:
        import torch
    except ImportError:
        # Try loading via omnipkg
        try:
            from omnipkg.loader import omnipkgLoader

            with omnipkgLoader("torch==2.2.0+cu121"):
                import torch
        except:
            safe_print("   âš ï¸  PyTorch not available, skipping GPU test")
            return True

    if not torch.cuda.is_available():
        safe_print("   âš ï¸  CUDA not available, skipping GPU test")
        return True

    if not torch.cuda.is_available():
        safe_print("   âš ï¸  CUDA not available, skipping GPU test")
        return True

    safe_print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    safe_print("â•‘  TEST 20: ğŸ”¥ GPU-RESIDENT ZERO-COPY PIPELINE                â•‘")
    safe_print("â•‘  Data NEVER leaves VRAM - Sub-microsecond latency!          â•‘")
    safe_print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    # Initialize daemon
    try:
        from omnipkg.isolation.worker_daemon import DaemonClient
        import subprocess
        import time

        client = DaemonClient()
        if not client.status().get("success"):
            safe_print("   âš™ï¸  Starting daemon...")
            subprocess.run(
                [sys.executable, "-m", "omnipkg.isolation.worker_daemon", "start"]
            )
            time.sleep(2)
    except ImportError as e:
        safe_print(f"   âŒ Failed to import daemon client: {e}")
        return False

    # FIXED: Use available bubbled versions that actually exist
    # Based on your system: 2.2.0+cu121, 2.0.1+cu118 are available
    specs = [
        "torch==2.2.0+cu121",  # Stage 1
        "torch==2.0.1+cu118",  # Stage 2
        "torch==2.2.0+cu121",  # Stage 3 (reuse)
    ]

    safe_print("   ğŸ“¦ Pipeline Stages:")
    safe_print(f"      Stage 1: {specs[0]}")
    safe_print(f"      Stage 2: {specs[1]}")
    safe_print(f"      Stage 3: {specs[2]}\n")

    # Create tensor on GPU
    try:
        pipeline_data = torch.randn(500, 250, device="cuda:0")
        safe_print(
            f"   ğŸ“¦ Input: {pipeline_data.shape} tensor on {pipeline_data.device}"
        )
        safe_print(f"   ğŸ“Š Size: {pipeline_data.numel() * 4 / 1024 / 1024:.2f} MB\n")
    except Exception as e:
        safe_print(f"   âŒ Failed to create GPU tensor: {e}")
        return False

    # Stage 1: ReLU activation
    safe_print("   ğŸ”´ Stage 1: PyTorch 2.2 (ReLU)...")
    stage1_code = """
import torch
result = torch.nn.functional.relu(tensor_in)
tensor_out.copy_(result)
"""

    try:
        stage1_start = time.perf_counter()
        stage1_out, meta1 = client.execute_cuda_ipc(
            specs[0],
            stage1_code,
            input_tensor=pipeline_data,
            output_shape=pipeline_data.shape,
            output_dtype="float32",
            python_exe=sys.executable,
        )
        stage1_time = (time.perf_counter() - stage1_start) * 1000
        safe_print(f"   âœ… Stage 1 complete: {stage1_time:.3f}ms (CUDA IPC)")
    except Exception as e:
        safe_print(f"   âŒ Stage 1 failed: {e}")
        import traceback

        safe_print(f"      {traceback.format_exc()[:500]}")
        return False

    # Stage 2: Sigmoid
    safe_print("\n   ğŸŸ¢ Stage 2: PyTorch 2.0 (Sigmoid)...")
    stage2_code = """
import torch
result = torch.sigmoid(tensor_in)
tensor_out.copy_(result)
"""

    try:
        stage2_start = time.perf_counter()
        stage2_out, meta2 = client.execute_cuda_ipc(
            specs[1],
            stage2_code,
            input_tensor=stage1_out,
            output_shape=stage1_out.shape,
            output_dtype="float32",
            python_exe=sys.executable,
        )
        stage2_time = (time.perf_counter() - stage2_start) * 1000
        safe_print(f"   âœ… Stage 2 complete: {stage2_time:.3f}ms (CUDA IPC)")
    except Exception as e:
        safe_print(f"   âŒ Stage 2 failed: {e}")
        import traceback

        safe_print(f"      {traceback.format_exc()[:500]}")
        return False

    # Stage 3: Tanh
    safe_print("\n   ğŸ”µ Stage 3: PyTorch 2.2 (Tanh)...")
    stage3_code = """
import torch
result = torch.tanh(tensor_in)
tensor_out.copy_(result)
"""

    try:
        stage3_start = time.perf_counter()
        stage3_out, meta3 = client.execute_cuda_ipc(
            specs[2],
            stage3_code,
            input_tensor=stage2_out,
            output_shape=stage2_out.shape,
            output_dtype="float32",
            python_exe=sys.executable,
        )
        stage3_time = (time.perf_counter() - stage3_start) * 1000
        safe_print(f"   âœ… Stage 3 complete: {stage3_time:.3f}ms (CUDA IPC)")
    except Exception as e:
        safe_print(f"   âŒ Stage 3 failed: {e}")
        import traceback

        safe_print(f"      {traceback.format_exc()[:500]}")
        return False

    # Results
    total_pipeline = stage1_time + stage2_time + stage3_time
    safe_print("\n" + "=" * 66)
    safe_print("   ğŸ“Š GPU-RESIDENT PIPELINE RESULTS")
    safe_print("=" * 66)
    safe_print(f"   Stage 1 (PyTorch 2.2 ReLU):    {stage1_time:>8.3f}ms")
    safe_print(f"   Stage 2 (PyTorch 2.0 Sigmoid): {stage2_time:>8.3f}ms")
    safe_print(f"   Stage 3 (PyTorch 2.2 Tanh):    {stage3_time:>8.3f}ms")
    safe_print(f"   {'â”€'*40}")
    safe_print(f"   Total Pipeline:    {total_pipeline:>8.3f}ms")
    safe_print(f"   Per-Stage Average: {total_pipeline/3:>8.3f}ms")

    # Verify output is still on GPU
    if stage3_out.is_cuda:
        safe_print(f"\n   âœ… Output tensor still on GPU: {stage3_out.device}")
        safe_print("   ğŸ”¥ Data NEVER left VRAM - Zero PCIe transfers!")
        safe_print("   ğŸŒŒ Multi-version pipeline: 2.2 â†’ 2.0 â†’ 2.2 via CUDA IPC!")
    else:
        safe_print("\n   âš ï¸  Output tensor moved to CPU (unexpected)")

    # Compare to CPU SHM (if Test 17 ran)
    safe_print("\n   ğŸ’¡ COMPARISON:")
    safe_print("      CPU SHM Pipeline (Test 17): ~17ms")
    safe_print(f"      GPU IPC Pipeline (Test 20): {total_pipeline:.1f}ms")

    if total_pipeline < 17:
        speedup = 17.0 / total_pipeline
        safe_print(f"      ğŸš€ Speedup: {speedup:.1f}x FASTER!")
    else:
        safe_print("      âš ï¸  GPU IPC slower than expected (daemon overhead)")
        safe_print("      ğŸ’¡ But data stayed in VRAM - no CPU copies!")

    safe_print("\n   ğŸ† VERDICT: GPU-RESIDENT PIPELINE OPERATIONAL!")
    safe_print("   ğŸ‰ THREE DIFFERENT PYTORCH VERSIONS, ONE GPU, ZERO COPIES!\n")

    return True


def chaos_test_21_gpu_resident_pipeline():
    """
     TEST 21: ğŸ”¥ GPU-RESIDENT MULTI-VERSION PIPELINE
    ğŸ“ PHASE 4: Zero-Copy Data Pipeline
     NOW ACTUALLY USES PyTorch 1.13's native CUDA IPC!
    """
    import time
    import sys

    safe_print(f"\n{'â•'*66}")
    safe_print("â•‘  TEST 21: ğŸ”¥ GPU-RESIDENT MULTI-VERSION PIPELINE           â•‘")
    safe_print("â•‘  PyTorch 1.13.1 with NATIVE CUDA IPC (True Zero-Copy!)     â•‘")
    safe_print(f"{'â•'*66}\n")

    from omnipkg.loader import omnipkgLoader

    safe_print("ğŸ“ PHASE 1: Configuration")
    safe_print("â”€" * 60)

    TORCH_VERSION = "torch==1.13.1+cu116"

    print(f"   PyTorch Version: {TORCH_VERSION}")
    safe_print(f"   ğŸ”¥ Loading client in {TORCH_VERSION} context...")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CRITICAL FIX: Keep entire test inside loader context!
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with omnipkgLoader(TORCH_VERSION, isolation_mode="overlay"):
        import torch

        if not torch.cuda.is_available():
            safe_print("âŒ CUDA not available - skipping test")
            return {"success": False, "reason": "No CUDA"}

        safe_print(f"   âœ… Client PyTorch: {torch.__version__}")

        from omnipkg.isolation.worker_daemon import DaemonClient

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Check available versions
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        safe_print("\nğŸ“ PHASE 1: Checking PyTorch Versions")
        safe_print("â”€" * 60)

        try:
            from omnipkg.core import OmnipkgCore

            core = OmnipkgCore()

            torch_versions = []
            for pkg_name, versions in core.kb.packages.items():
                if pkg_name == "torch":
                    for ver_str in versions.keys():
                        torch_versions.append(ver_str)

            print(f"Found {len(torch_versions)} PyTorch versions:")
            for v in torch_versions:
                marker = " ğŸ”¥" if v.startswith("1.13") else ""
                print(f"   - torch=={v}{marker}")
        except Exception as e:
            safe_print(f"   âš ï¸  Could not query knowledge base: {e}")
            torch_versions = ["1.13.1+cu116", "2.0.1+cu118", "2.2.0+cu121"]

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # PHASE 2: Configure Pipeline
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        safe_print("\nğŸ“ PHASE 2: Configuring Pipeline")
        safe_print("â”€" * 60)
        safe_print(f"ğŸ“ CLIENT PyTorch version: {torch.__version__}")

        stage_specs = []

        # Find PyTorch 1.13 for native IPC
        torch_1x = next((v for v in torch_versions if v.startswith("1.13")), None)

        if torch_1x:
            stage_specs.append(
                ("ğŸ”¥ Stage 1 (ReLU)", f"torch=={torch_1x}", "relu", "NATIVE IPC")
            )
            safe_print(f"   âœ… Using torch=={torch_1x} (NATIVE CUDA IPC!)")
        else:
            stage_specs.append(
                ("ğŸ”´ Stage 1 (ReLU)", "torch==2.2.0+cu121", "relu", "HYBRID")
            )
            safe_print("   âš ï¸  PyTorch 1.13 not available, using hybrid mode")

        # Add other stages
        if len(torch_versions) >= 2:
            other_versions = [v for v in torch_versions if not v.startswith("1.13")][:2]
            stage_specs.append(
                (
                    "ğŸŸ¢ Stage 2 (Sigmoid)",
                    f"torch=={other_versions[0]}",
                    "sigmoid",
                    "HYBRID",
                )
            )
            if len(other_versions) > 1:
                stage_specs.append(
                    (
                        "ğŸ”µ Stage 3 (Tanh)",
                        f"torch=={other_versions[1]}",
                        "tanh",
                        "HYBRID",
                    )
                )

        print("\n   Pipeline Configuration:")
        for name, spec, op, mode in stage_specs:
            print(f"   {name}: {spec} ({mode})")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # PHASE 3: Execute Pipeline
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        safe_print("\nğŸ“ PHASE 3: Executing GPU Pipeline")
        safe_print("â”€" * 60)

        client = DaemonClient()

        # Create test tensor
        device = torch.device("cuda:0")
        pipeline_data = torch.randn(500, 250, device=device, dtype=torch.float32)

        safe_print(f"\nğŸ“¦ Input: {pipeline_data.shape} tensor on {device}")
        safe_print(f"ğŸ“Š Size: {pipeline_data.numel() * 4 / 1024 / 1024:.2f} MB")
        safe_print(f"ğŸ”¢ Checksum: {pipeline_data.sum().item():.2f}")

        stage_codes = {
            "relu": "tensor_out[:] = torch.relu(tensor_in)",
            "sigmoid": "tensor_out[:] = torch.sigmoid(tensor_in)",
            "tanh": "tensor_out[:] = torch.tanh(tensor_in)",
        }

        stage_times = []
        current_tensor = pipeline_data
        native_ipc_used = False

        for i, (name, spec, operation, mode) in enumerate(stage_specs):
            print(f"\n{name}: Processing...")
            print(f"   PyTorch: {spec}")
            print(f"   Mode: {mode}")

            stage_start = time.perf_counter()

            try:
                # Force native IPC mode if this stage is marked for it
                ipc_mode = "pytorch_native" if mode == "NATIVE IPC" else "auto"

                result_tensor, response = client.execute_cuda_ipc(
                    spec,
                    stage_codes[operation],
                    input_tensor=current_tensor,
                    output_shape=current_tensor.shape,
                    output_dtype="float32",
                    python_exe=sys.executable,
                    ipc_mode=ipc_mode,  # â† ADD THIS!
                )

                stage_time = (time.perf_counter() - stage_start) * 1000
                stage_times.append(stage_time)

                actual_method = response.get("cuda_method", "unknown")
                if actual_method == "native_ipc":
                    native_ipc_used = True
                    safe_print(
                        f"âœ… {name} complete: {stage_time:.3f}ms (NATIVE CUDA IPC! ğŸ”¥)"
                    )
                else:
                    safe_print(f"âœ… {name} complete: {stage_time:.3f}ms (hybrid)")

                print(f"   Checksum: {result_tensor.sum().item():.2f}")
                current_tensor = result_tensor

            except Exception as e:
                safe_print(f"âŒ {name} failed: {e}")
                import traceback

                traceback.print_exc()
                return {"success": False, "error": str(e)}

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # RESULTS (still inside loader context)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        total_time = sum(stage_times)
        avg_time = total_time / len(stage_times)

        safe_print(f"\n{'â•'*66}")
        safe_print("ğŸ“Š GPU-RESIDENT PIPELINE RESULTS")
        safe_print(f"{'â•'*66}\n")

        for i, (name, spec, unused, mode) in enumerate(stage_specs):
            icon = "ğŸ”¥" if mode == "NATIVE IPC" else "ğŸ”„"
            print(f"{icon} {name:<40} {stage_times[i]:>8.3f}ms")
            safe_print(f"  â””â”€ {spec:<38}")

        safe_print("â”€" * 66)
        print(f"{'Total Pipeline:':<40} {total_time:>8.3f}ms")
        print(f"{'Per-Stage Average:':<40} {avg_time:>8.3f}ms")

        safe_print(f"\nâœ… Output tensor still on GPU: {current_tensor.device}")
        safe_print(f"ğŸ”¢ Final checksum: {current_tensor.sum().item():.2f}")

        if native_ipc_used:
            safe_print("\nğŸ† NATIVE CUDA IPC USED! TRUE ZERO-COPY ACHIEVED!")
            safe_print("   ğŸ“Š Stage 1 had ZERO PCIe transfers")
        else:
            safe_print("\nâš ï¸  Native CUDA IPC not available")
            safe_print("   ğŸ’¡ Install torch==1.13.1+cu116 for true zero-copy")

        safe_print("\nğŸ’¡ PERFORMANCE:")
        print("   CPU SHM Pipeline (Test 17): ~17ms")
        print("   GPU Hybrid (Test 20): ~13ms")
        print(f"   This test: {total_time:.1f}ms")

        return {
            "success": True,
            "total_time_ms": total_time,
            "native_ipc_used": native_ipc_used,
            "avg_stage_ms": avg_time,
        }


def chaos_test_22_complete_ipc_benchmark():
    """
    TEST 22: ğŸ”¥ COMPLETE IPC MODE BENCHMARK

    Compares ALL 4 execution modes:
    1. Universal CUDA IPC - Pure GPU, zero-copy (ctypes)
    2. PyTorch Native IPC - Framework-managed GPU IPC
    3. Hybrid Mode - CPU SHM + GPU copies (2 PCIe per stage)
    4. CPU SHM Baseline - Pure CPU, zero-copy (no GPU)

    Tests the same 3-stage pipeline across all modes with proper warmup.
    """
    import sys
    import time

    safe_print(f"\n{'â•'*66}")
    safe_print("â•‘  TEST 22: ğŸ”¥ COMPLETE IPC MODE BENCHMARK              â•‘")
    safe_print("â•‘  Same Pipeline Ã— 4 Different Execution Modes          â•‘")
    safe_print(f"{'â•'*66}\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SETUP: Load PyTorch 1.13.1 for client + workers
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    from omnipkg.loader import omnipkgLoader

    TORCH_VERSION = "torch==1.13.1+cu116"

    safe_print("ğŸ“ CONFIGURATION")
    safe_print("â”€" * 60)
    print(f"   PyTorch Version: {TORCH_VERSION}")
    print("   Pipeline: 3 stages (ReLU â†’ Sigmoid â†’ Tanh)")
    print("   Testing 4 execution modes\n")

    with omnipkgLoader(TORCH_VERSION, isolation_mode="overlay"):
        import torch

        if not torch.cuda.is_available():
            safe_print("âŒ CUDA not available - skipping test")
            return {"success": False, "reason": "No CUDA"}

        safe_print(f"   âœ… Client PyTorch: {torch.__version__}")

        from omnipkg.isolation.worker_daemon import DaemonClient

        client = DaemonClient()
        device = torch.device("cuda:0")

        # Create test data: 1000x500 = 2MB float32 tensor
        pipeline_data = torch.randn(1000, 500, device=device, dtype=torch.float32)

        safe_print(f"   ğŸ“¦ Input: {pipeline_data.shape} tensor on {device}")
        safe_print(f"   ğŸ“Š Size: {pipeline_data.numel() * 4 / 1024 / 1024:.2f} MB\n")

        # Define 3-stage pipeline
        stage_specs = [
            ("Stage 1: ReLU", TORCH_VERSION, "relu"),
            ("Stage 2: Sigmoid", TORCH_VERSION, "sigmoid"),
            ("Stage 3: Tanh", TORCH_VERSION, "tanh"),
        ]

        # GPU operations
        gpu_stage_codes = {
            "relu": "tensor_out[:] = torch.relu(tensor_in)",
            "sigmoid": "tensor_out[:] = torch.sigmoid(tensor_in)",
            "tanh": "tensor_out[:] = torch.tanh(tensor_in)",
        }

        # CPU operations (numpy equivalent)
        cpu_stage_codes = {
            "relu": "arr_out[:] = np.maximum(arr_in, 0)",
            "sigmoid": "arr_out[:] = 1 / (1 + np.exp(-arr_in))",
            "tanh": "arr_out[:] = np.tanh(arr_in)",
        }

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # TEST MODES
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        modes = [
            {
                "key": "universal",
                "name": "Universal CUDA IPC",
                "desc": "Pure GPU, zero-copy (ctypes)",
                "icon": "ğŸ”¥",
                "type": "gpu",
            },
            {
                "key": "pytorch_native",
                "name": "PyTorch Native IPC",
                "desc": "Framework-managed GPU IPC",
                "icon": "ğŸ",
                "type": "gpu",
            },
            {
                "key": "hybrid",
                "name": "Hybrid Mode",
                "desc": "CPU SHM + GPU copies",
                "icon": "ğŸ”„",
                "type": "gpu",
            },
            {
                "key": "cpu_shm",
                "name": "CPU SHM Baseline",
                "desc": "Pure CPU, zero-copy",
                "icon": "ğŸ’¾",
                "type": "cpu",
            },
        ]

        results = {}

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # WARMUP PHASE (5 iterations each mode)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        safe_print("ğŸ“ WARMUP PHASE (5 iterations per mode)")
        safe_print("â”€" * 60)

        for mode in modes:
            print(f"\n{mode['icon']} Warming up: {mode['name']}")

            try:
                if mode["type"] == "cpu":
                    # CPU mode warmup
                    cpu_data = pipeline_data.cpu().numpy()

                    for i in range(5):
                        curr_data = cpu_data
                        for unused, spec, op in stage_specs:
                            curr_data, unused = client.execute_zero_copy(
                                spec,
                                cpu_stage_codes[op],
                                input_array=curr_data,
                                output_shape=curr_data.shape,
                                output_dtype=curr_data.dtype,
                                python_exe=sys.executable,
                            )
                else:
                    # GPU mode warmup
                    for i in range(5):
                        curr = pipeline_data
                        for unused, spec, op in stage_specs:
                            curr, unused = client.execute_cuda_ipc(
                                spec,
                                gpu_stage_codes[op],
                                input_tensor=curr,
                                output_shape=curr.shape,
                                output_dtype="float32",
                                python_exe=sys.executable,
                                ipc_mode=mode["key"],
                            )

                safe_print("   âœ… Warmup complete")

            except Exception as e:
                safe_print(f"   âŒ Warmup failed: {e}")
                results[mode["key"]] = {"error": str(e), "skipped": True}

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # BENCHMARK PHASE (20 iterations each mode)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        safe_print(f"\n\n{'â•'*66}")
        safe_print("ğŸ“ BENCHMARK PHASE (20 iterations per mode)")
        safe_print("â•" * 66)

        for mode in modes:
            if mode["key"] in results and results[mode["key"]].get("skipped"):
                continue

            print(f"\n{mode['icon']} Testing: {mode['name']}")
            print(f"   {mode['desc']}")
            safe_print("   " + "â”€" * 60)

            run_times = []

            try:
                for run in range(20):
                    if mode["type"] == "cpu":
                        # CPU mode benchmark
                        cpu_data = pipeline_data.cpu().numpy()

                        run_start = time.perf_counter()
                        curr_data = cpu_data

                        for unused, spec, op in stage_specs:
                            curr_data, unused = client.execute_zero_copy(
                                spec,
                                cpu_stage_codes[op],
                                input_array=curr_data,
                                output_shape=curr_data.shape,
                                output_dtype=curr_data.dtype,
                                python_exe=sys.executable,
                            )

                        run_time = (time.perf_counter() - run_start) * 1000
                        run_times.append(run_time)

                    else:
                        # GPU mode benchmark
                        run_start = time.perf_counter()
                        curr = pipeline_data

                        for unused, spec, op in stage_specs:
                            curr, unused = client.execute_cuda_ipc(
                                spec,
                                gpu_stage_codes[op],
                                input_tensor=curr,
                                output_shape=curr.shape,
                                output_dtype="float32",
                                python_exe=sys.executable,
                                ipc_mode=mode["key"],
                            )

                        run_time = (time.perf_counter() - run_start) * 1000
                        run_times.append(run_time)

                    # Show progress for first 5 runs
                    if run < 5:
                        print(f"   Run {run+1:2d}: {run_time:.3f}ms")

                # Calculate statistics
                if run_times:
                    avg = sum(run_times) / len(run_times)
                    min_time = min(run_times)
                    max_time = max(run_times)
                    stddev = (
                        sum((x - avg) ** 2 for x in run_times) / len(run_times)
                    ) ** 0.5

                    results[mode["key"]] = {
                        "times": run_times,
                        "avg": avg,
                        "min": min_time,
                        "max": max_time,
                        "stddev": stddev,
                        "name": mode["name"],
                        "icon": mode["icon"],
                        "type": mode["type"],
                    }

                    safe_print("\n   ğŸ“Š Statistics:")
                    print(f"      Average: {avg:.3f}ms")
                    print(f"      Best:    {min_time:.3f}ms")
                    print(f"      Worst:   {max_time:.3f}ms")
                    print(f"      Stddev:  {stddev:.3f}ms")

            except Exception as e:
                safe_print(f"   âŒ Benchmark failed: {e}")
                results[mode["key"]] = {"error": str(e)}

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # FINAL COMPARISON
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        safe_print(f"\n\n{'â•'*66}")
        safe_print("ğŸ“Š FINAL RESULTS - IPC MODE COMPARISON")
        safe_print(f"{'â•'*66}\n")

        # Filter valid results
        valid_results = {k: v for k, v in results.items() if "times" in v}

        if not valid_results:
            safe_print("âŒ No valid results to compare")
            return {"success": False, "error": "All modes failed"}

        # Sort by best time
        sorted_modes = sorted(valid_results.items(), key=lambda x: x[1]["min"])

        # Show ranking
        safe_print("ğŸ† RANKING (by best time):")
        safe_print("â”€" * 60)

        medals = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "  "]

        for i, (key, data) in enumerate(sorted_modes):
            medal = medals[i] if i < 4 else "  "
            print(
                f"{medal} {data['icon']} {data['name']:<25} "
                f"{data['min']:.3f}ms (avg: {data['avg']:.3f}ms Â± {data['stddev']:.2f}ms)"
            )

        # Show speedup comparisons
        if len(sorted_modes) > 1:
            safe_print("\nğŸ’¡ SPEEDUP vs FASTEST:")
            safe_print("â”€" * 60)

            fastest_time = sorted_modes[0][1]["min"]

            for key, data in sorted_modes[1:]:
                speedup = data["min"] / fastest_time
                slower = data["min"] - fastest_time
                print(
                    f"   {data['icon']} {data['name']:<25} "
                    f"{speedup:.2f}x slower (+{slower:.3f}ms)"
                )

        # Show winner and analysis
        winner_key = sorted_modes[0][0]
        winner = sorted_modes[0][1]

        safe_print(f"\n{'â•'*66}")
        safe_print(f"ğŸ† WINNER: {winner['name'].upper()}")
        safe_print(f"{'â•'*66}")
        print(f"   Best time: {winner['min']:.3f}ms")
        print(f"   Average:   {winner['avg']:.3f}ms")
        print(f"   Stddev:    {winner['stddev']:.3f}ms")

        # Technical analysis
        safe_print("\nğŸ’¡ TECHNICAL ANALYSIS:")
        safe_print("â”€" * 60)

        if winner_key == "universal":
            safe_print("   âœ… Universal IPC is fastest - pure CUDA IPC wins!")
            safe_print("   ğŸš€ Zero-copy GPU transfers via ctypes")
            safe_print("   ğŸ“Œ No PyTorch dependency for IPC layer")
            safe_print("   ğŸ’¡ This is the DEFAULT mode (optimal choice)")

        elif winner_key == "pytorch_native":
            safe_print("   ğŸ PyTorch Native IPC is fastest!")
            safe_print("   ğŸš€ Framework-managed zero-copy transfers")
            safe_print("   ğŸ“Œ Uses PyTorch 1.x _share_cuda_() API")
            safe_print("   ğŸ’¡ Consider setting ipc_mode='pytorch_native' as default")

        elif winner_key == "hybrid":
            safe_print("   ğŸ”„ Hybrid mode is fastest - surprising!")
            safe_print("   ğŸ“Š This means: PCIe transfer < GPU IPC setup overhead")
            safe_print("   ğŸ’¡ For this workload, copying data is faster than IPC")
            safe_print("   âš ï¸  Might indicate GPU IPC driver issues")

        elif winner_key == "cpu_shm":
            safe_print("   ğŸ’¾ CPU-only is fastest - GPU overhead too high!")
            safe_print("   ğŸ“Š For this workload size, CPU is more efficient")
            safe_print("   ğŸ’¡ GPU transfers + kernel launches exceed CPU compute time")
            safe_print("   âš ï¸  Consider larger workloads to amortize GPU overhead")

        # Method explanations
        safe_print("\nğŸ“š METHOD EXPLANATIONS:")
        safe_print("â”€" * 60)
        safe_print(
            "   ğŸ”¥ Universal IPC:     Pure CUDA IPC (ctypes), works with any PyTorch"
        )
        safe_print("   ğŸ PyTorch Native:    Framework-managed, PyTorch 1.x only")
        safe_print("   ğŸ”„ Hybrid:            CPU SHM + 2 GPU copies per stage")
        safe_print("   ğŸ’¾ CPU SHM:           Pure CPU compute, zero-copy (baseline)")

        # Performance summary
        gpu_modes = {k: v for k, v in valid_results.items() if v["type"] == "gpu"}
        cpu_modes = {k: v for k, v in valid_results.items() if v["type"] == "cpu"}

        if gpu_modes and cpu_modes:
            safe_print("\nğŸ¯ GPU vs CPU COMPARISON:")
            safe_print("â”€" * 60)

            best_gpu = min(gpu_modes.values(), key=lambda x: x["min"])
            best_cpu = min(cpu_modes.values(), key=lambda x: x["min"])

            if best_gpu["min"] < best_cpu["min"]:
                speedup = best_cpu["min"] / best_gpu["min"]
                safe_print(
                    f"   ğŸš€ Best GPU ({best_gpu['name']}) is {speedup:.2f}x faster than CPU"
                )
                print(f"      GPU: {best_gpu['min']:.3f}ms")
                print(f"      CPU: {best_cpu['min']:.3f}ms")
            else:
                ratio = best_gpu["min"] / best_cpu["min"]
                safe_print(f"   âš ï¸  CPU is {ratio:.2f}x faster than best GPU mode!")
                print(f"      CPU: {best_cpu['min']:.3f}ms")
                print(f"      GPU: {best_gpu['min']:.3f}ms")
                safe_print("   ğŸ’¡ Workload too small to benefit from GPU")

        print("=" * 66 + "\n")

        return {
            "success": True,
            "results": results,
            "winner": winner_key,
            "best_time_ms": winner["min"],
        }


def chaos_test_23_grand_unified_benchmark():
    """
    TEST 23: ğŸ† THE GRAND UNIFIED BENCHMARK

    The Final Showdown:
    1. ğŸ¢ The "Lame" Way: Traditional Subprocess + Pickle (The standard industry approach)
    2. ğŸ’¾ The "Smart" Way: CPU Shared Memory (OmniPKG Zero-Copy)
    3. ğŸ”¥ The "God" Mode: Universal CUDA IPC (OmniPKG Zero-Copy GPU)

    Runs a multi-version pipeline (PyTorch 1.13 -> 2.0 -> 2.1) across all modes.
    """
    import sys
    import time
    import subprocess
    import pickle
    import os
    import numpy as np

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SETUP & VISUALS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def safe_print(msg):
        sys.stdout.write(msg + "\n")
        sys.stdout.flush()

    safe_print(f"\n{'â•'*70}")
    safe_print("â•‘  TEST 23: ğŸ† GRAND UNIFIED BENCHMARK (The Final Boss)            â•‘")
    safe_print("â•‘  Comparing Architecture Generations:                             â•‘")
    safe_print("â•‘  1. ğŸ¢ Process Forking (Standard)                                â•‘")
    safe_print("â•‘  2. ğŸ’¾ CPU Zero-Copy (OmniPKG v1)                                â•‘")
    safe_print("â•‘  3. ğŸ”¥ GPU Direct IPC (OmniPKG v2)                               â•‘")
    safe_print(f"{'â•'*70}\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CONFIGURATION: THE MULTIVERSE PIPELINE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # We use 3 different PyTorch versions to force true isolation
    STAGES = [
        {"name": "Stage 1 (ReLU)", "spec": "torch==1.13.1+cu116", "op": "relu"},
        {"name": "Stage 2 (Sigmoid)", "spec": "torch==2.0.1+cu118", "op": "sigmoid"},
        {"name": "Stage 3 (Tanh)", "spec": "torch==2.1.0", "op": "tanh"},
    ]

    # Data Size: 1000x1000 float32 (4MB) - Big enough to hurt pickling, small enough for IPC
    SHAPE = (1000, 1000)
    DTYPE = "float32"

    # Generate Input Data
    try:
        from omnipkg.loader import omnipkgLoader

        with omnipkgLoader("torch==1.13.1+cu116", quiet=True):
            import torch

            if not torch.cuda.is_available():
                return {"success": False, "reason": "No CUDA"}

            # CPU Data for Baselines
            input_cpu = np.random.randn(*SHAPE).astype(np.float32)
            # GPU Data for God Mode
            device = torch.device("cuda:0")
            input_gpu = torch.as_tensor(input_cpu, device=device)

            safe_print(
                f"   ğŸ“¦ Payload: {SHAPE} Matrix ({input_cpu.nbytes/1024/1024:.2f} MB)"
            )
            safe_print(
                f"   ğŸŒŠ Pipeline: {STAGES[0]['spec']} â†’ {STAGES[1]['spec']} â†’ {STAGES[2]['spec']}\n"
            )
    except Exception as e:
        safe_print(f"âŒ Setup failed: {e}")
        return {"success": False}

    results = {}

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MODE 1: TRADITIONAL SUBPROCESS (The "Lame" Way)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("ğŸ¢ MODE 1: TRADITIONAL SUBPROCESS (Pickle + Fork)")
    safe_print("   - Strategy: Spawn new python process for every stage")
    safe_print("   - Data: Serialize to disk/pipe (Pickle)")
    safe_print("   - Status: This will be painful to watch...")
    safe_print("   " + "â”€" * 60)

    t_start = time.perf_counter()

    # We only do 1 run because it's so slow
    current_data = input_cpu

    try:
        for i, stage in enumerate(STAGES):
            # Write data to temp file (simulate disk/pipe transfer)
            tmp_in = f"temp_in_{i}.pkl"
            tmp_out = f"temp_out_{i}.pkl"

            with open(tmp_in, "wb") as f:
                pickle.dump(current_data, f)

            # The "Lame" Script
            code = f"""
import pickle
import numpy as np
from omnipkg.loader import omnipkgLoader
# We have to load the environment every time!
with omnipkgLoader("{stage['spec']}", quiet=True):
    import torch
    
    with open("{tmp_in}", "rb") as f:
        data = pickle.load(f)
    
    tensor = torch.from_numpy(data)
    
    if "{stage['op']}" == "relu":
        res = torch.relu(tensor)
    elif "{stage['op']}" == "sigmoid":
        res = torch.sigmoid(tensor)
    elif "{stage['op']}" == "tanh":
        res = torch.tanh(tensor)
        
    with open("{tmp_out}", "wb") as f:
        pickle.dump(res.numpy(), f)
"""
            # Execute Subprocess
            proc = subprocess.run([sys.executable, "-c", code], capture_output=True)
            if proc.returncode != 0:
                raise Exception(f"Subprocess failed: {proc.stderr.decode()}")

            # Read back
            with open(tmp_out, "rb") as f:
                current_data = pickle.load(f)

            # Cleanup
            if os.path.exists(tmp_in):
                os.remove(tmp_in)
            if os.path.exists(tmp_out):
                os.remove(tmp_out)

            safe_print(f"   ğŸ¢ Stage {i+1} ({stage['spec']}) complete")

        t_end = time.perf_counter()
        time_lame = (t_end - t_start) * 1000
        results["lame"] = time_lame
        safe_print(f"   â±ï¸  Total Time: {time_lame:.2f}ms")

    except Exception as e:
        safe_print(f"   âŒ Failed: {e}")
        results["lame"] = 999999

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SETUP DAEMON FOR HIGH SPEED MODES
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("\nâš™ï¸  Booting OmniPKG Daemon Workers (One-time setup)...")
    from omnipkg.isolation.worker_daemon import DaemonClient

    client = DaemonClient()

    # Warmup workers to make benchmark fair (remove boot time)
    for stage in STAGES:
        client.execute_zero_copy(
            stage["spec"], "pass", input_cpu, SHAPE, DTYPE, python_exe=sys.executable
        )
    safe_print("   âœ… Workers warm and ready\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MODE 2: CPU SHARED MEMORY (The "Smart" Way)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("ğŸ’¾ MODE 2: CPU SHARED MEMORY (OmniPKG v1)")
    safe_print("   - Strategy: Persistent Workers + Shared Memory Ring Buffer")
    safe_print("   - Data: Zero-copy pointer passing")
    safe_print("   " + "â”€" * 60)

    cpu_times = []

    cpu_code_map = {
        "relu": "arr_out[:] = np.maximum(arr_in, 0)",
        "sigmoid": "arr_out[:] = 1 / (1 + np.exp(-arr_in))",
        "tanh": "arr_out[:] = np.tanh(arr_in)",
    }

    for run in range(5):  # 5 runs
        t_start = time.perf_counter()
        curr = input_cpu

        for stage in STAGES:
            curr, unused = client.execute_zero_copy(
                stage["spec"],
                cpu_code_map[stage["op"]],
                input_array=curr,
                output_shape=SHAPE,
                output_dtype=DTYPE,
                python_exe=sys.executable,
            )

        cpu_times.append((time.perf_counter() - t_start) * 1000)

    avg_cpu = sum(cpu_times) / len(cpu_times)
    results["cpu"] = avg_cpu
    safe_print(f"   âœ… Average: {avg_cpu:.3f}ms")
    safe_print(f"   ğŸš€ Speedup vs Lame: {results['lame']/avg_cpu:.1f}x")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MODE 3: UNIVERSAL CUDA IPC (The "God" Mode)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("\nğŸ”¥ MODE 3: UNIVERSAL CUDA IPC (God Mode)")
    safe_print("   - Strategy: GPU Pointers via Ctypes -> NV Driver")
    safe_print("   - Data: STAYS ON VRAM. ZERO PCIe TRANSFERS.")
    safe_print("   " + "â”€" * 60)

    gpu_times = []

    gpu_code_map = {
        "relu": "tensor_out[:] = torch.relu(tensor_in)",
        "sigmoid": "tensor_out[:] = torch.sigmoid(tensor_in)",
        "tanh": "tensor_out[:] = torch.tanh(tensor_in)",
    }

    # Ensure input is on GPU in the main process context
    with omnipkgLoader("torch==1.13.1+cu116", quiet=True):
        import torch

        gpu_tensor = input_gpu

        for run in range(10):  # 10 runs because it's so fast
            t_start = time.perf_counter()
            curr = gpu_tensor

            for stage in STAGES:
                curr, meta = client.execute_cuda_ipc(
                    stage["spec"],
                    gpu_code_map[stage["op"]],
                    input_tensor=curr,
                    output_shape=SHAPE,
                    output_dtype=DTYPE,
                    python_exe=sys.executable,
                    ipc_mode="universal",
                )

            torch.cuda.synchronize()  # Wait for GPU to finish for fair timing
            gpu_times.append((time.perf_counter() - t_start) * 1000)

    avg_gpu = sum(gpu_times) / len(gpu_times)
    results["gpu"] = avg_gpu
    safe_print(f"   âœ… Average: {avg_gpu:.3f}ms")
    safe_print(f"   ğŸš€ Speedup vs Lame: {results['lame']/avg_gpu:.1f}x")
    safe_print(f"   ğŸš€ Speedup vs CPU:  {results['cpu']/avg_gpu:.1f}x")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ† FINAL SCOREBOARD
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    safe_print("\n\n")
    safe_print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    safe_print("ğŸ“Š FINAL RESULTS - THE GRAND UNIFIED BENCHMARK")
    safe_print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    safe_print(f"{'STRATEGY':<30} | {'TIME (ms)':<12} | {'MULTIPLIER':<15}")
    safe_print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # Sort for dramatic effect (Slowest first)
    rows = [
        ("ğŸ¢ Traditional Process", results["lame"], "1.0x (Baseline)"),
        (
            "ğŸ’¾ OmniPKG CPU (SHM)",
            results["cpu"],
            f"{results['lame']/results['cpu']:.1f}x FASTER",
        ),
        (
            "ğŸ”¥ OmniPKG GPU (IPC)",
            results["gpu"],
            f"{results['lame']/results['gpu']:.1f}x FASTER",
        ),
    ]

    for name, t, mult in rows:
        safe_print(f"{name:<30} | {t:<12.3f} | {mult}")

    safe_print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    # Analysis
    safe_print("\nğŸ’¡ CONCLUSION:")
    safe_print(
        f"   1. The 'Traditional' way is {results['lame']/1000:.2f} seconds per inference."
    )
    safe_print("      - Completely unusable for real-time applications.")
    safe_print(f"   2. OmniPKG CPU is {(results['lame']/results['cpu']):.0f}x faster.")
    safe_print("      - Viable for production.")
    safe_print(f"   3. OmniPKG GPU is {(results['lame']/results['gpu']):.0f}x faster.")
    safe_print(
        f"      - This is {results['cpu']/results['gpu']:.1f}x faster than even the optimized CPU mode."
    )
    safe_print(
        f"      - 4MB of data moved through 3 PyTorch versions in {avg_gpu:.2f}ms."
    )

    safe_print("\nğŸ† WINNER: UNIVERSAL CUDA IPC")

    return {"success": True, "results": results}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ® INTERACTIVE MENU SYSTEM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ALL_TESTS = [
    chaos_test_1_version_tornado,
    chaos_test_2_dependency_inception,
    chaos_test_3_framework_battle_royale,
    chaos_test_4_memory_madness,
    chaos_test_5_race_condition_roulette,
    chaos_test_6_version_time_machine,
    chaos_test_7_dependency_jenga,
    chaos_test_8_quantum_superposition,
    chaos_test_9_import_hell,
    chaos_test_10_grand_finale,
    chaos_test_11_tensorflow_resurrection,
    chaos_test_12_jax_vs_torch_mortal_kombat,
    chaos_test_13_pytorch_lightning_storm,
    chaos_test_14_circular_dependency_hell,
    chaos_test_15_isolation_strategy_benchmark,
    chaos_test_16_nested_reality_hell,
    chaos_test_17_triple_python_multiverse,
    chaos_test_18_worker_pool_drag_race,  # <-- ADD THIS
    chaos_test_19_zero_copy_hft,
    chaos_test_20_gpu_resident_pipeline,
    chaos_test_21_gpu_resident_pipeline,
    chaos_test_22_complete_ipc_benchmark,
    chaos_test_23_grand_unified_benchmark,
]


def get_test_name(func):
    return func.__name__.replace("chaos_test_", "").replace("_", " ").title()


def select_tests_interactively():
    print_chaos_header()
    safe_print("ğŸ“ AVAILABLE CHAOS SCENARIOS:")
    print("=" * 60)
    safe_print("   [0] ğŸ”¥ RUN ALL TESTS (The Full Experience)")
    print("-" * 60)
    for i, test_func in enumerate(ALL_TESTS, 1):
        print(f"   [{i}] {get_test_name(test_func)}")
    print("=" * 60)
    safe_print("\nğŸ’¡ Tip: Type numbers separated by spaces (e.g. '1 3 5').")

    try:
        sys.stdout.flush()
        selection = input("\nğŸ‘‰ Choose tests [0]: ").strip()
    except (EOFError, KeyboardInterrupt):
        return ALL_TESTS

    if not selection or selection == "0" or selection.lower() == "all":
        return ALL_TESTS

    selected_tests = []
    try:
        parts = selection.replace(",", " ").split()
        indices = [int(x) for x in parts if x.strip().isdigit()]
        for idx in indices:
            if idx == 0:
                return ALL_TESTS
            if 1 <= idx <= len(ALL_TESTS):
                selected_tests.append(ALL_TESTS[idx - 1])
    except ValueError:
        return ALL_TESTS

    return selected_tests if selected_tests else ALL_TESTS


def run_chaos_suite(tests_to_run=None):
    if tests_to_run is None:
        tests_to_run = ALL_TESTS
    if not tests_to_run:
        return True

    results = []
    safe_print(f"\nğŸš€ Launching {len(tests_to_run)} chaos scenarios...\n")

    for i, test in enumerate(tests_to_run, 1):
        name = get_test_name(test)
        safe_print(f"\nğŸ§ª TEST {i}/{len(tests_to_run)}: {name}")
        safe_print("â”€" * 66)
        try:
            test()
            results.append(("âœ…", name))
            safe_print(f"âœ… {name} - PASSED")
        except Exception as e:
            results.append(("âŒ", name))
            safe_print(f"âŒ {name} - FAILED: {str(e)}")
        time.sleep(0.5)

    print("\n" + "=" * 66)
    safe_print("   ğŸ“Š DETAILED RESULTS:")
    safe_print("â”€" * 66)
    for status, name in results:
        print(f"   {status} {name}")

    passed = sum(1 for result in results if result[0] == "âœ…")

    safe_print("â”€" * 66)
    safe_print(f"\n   âœ… Tests Passed: {passed}/{len(tests_to_run)}")

    if passed == len(tests_to_run):
        safe_print("\n   ğŸ† System Status: GODLIKE")
    else:
        safe_print("\n   ğŸ©¹ System Status: WOUNDED")
    print("=" * 66 + "\n")

    return passed == len(tests_to_run)


if __name__ == "__main__":
    try:
        if os.environ.get("OMNIPKG_REEXEC_COUNT"):
            run_chaos_suite(ALL_TESTS)
        else:
            selected = select_tests_interactively()
            if selected:
                run_chaos_suite(selected)

    except KeyboardInterrupt:
        safe_print("\n\nâš ï¸  CHAOS INTERRUPTED BY USER!")
    except ProcessCorruptedException as e:
        safe_print(f"\nâ˜¢ï¸   CATASTROPHIC CORRUPTION: {e}")
        # Re-exec logic omitted for brevity in this cleaned version
        sys.exit(1)
    except Exception as e:
        import traceback

        safe_print(f"\nğŸ’¥ CHAOS FAILURE: {e}")
        traceback.print_exc()
